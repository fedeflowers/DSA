{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Essential LeetCode Problem Patterns for Amazon SDE I Interviews\n",
    "\n",
    "Preparing for an Amazon SDE I interview requires focusing on high-yield coding problem patterns. Amazon interview questions often revolve around fundamental data structures (arrays, strings, trees, graphs, etc.) and classic algorithmic techniques. Below is a comprehensive study guide of the **essential LeetCode problem patterns**, organized by priority (most frequent topics at Amazon appear first) ([Amazon Software Development Engineer Interview (questions, process, prep) - IGotAnOffer](https://igotanoffer.com/blogs/tech/amazon-software-development-engineer-interview#:~:text=1,of%20questions%2C%20least%20frequent)). For each pattern, we outline **when to apply it, variations, and classic LeetCode examples** (with links) to practice.\n",
    "\n",
    "## Key Topics and Frequency at Amazon\n",
    "\n",
    "Amazon most frequently asks questions involving **tree/graph traversals** and **array/string manipulations**, followed by other structures ([Amazon Software Development Engineer Interview (questions, process, prep) - IGotAnOffer](https://igotanoffer.com/blogs/tech/amazon-software-development-engineer-interview#:~:text=1,of%20questions%2C%20least%20frequent)). The approximate distribution of coding question topics is:\n",
    "\n",
    "| Topic                     | Frequency in Amazon Interviews ([Amazon Software Development Engineer Interview (questions, process, prep) - IGotAnOffer](https://igotanoffer.com/blogs/tech/amazon-software-development-engineer-interview#:~:text=1,of%20questions%2C%20least%20frequent)) |\n",
    "|---------------------------|----------------------------------------------|\n",
    "| **Graphs / Trees**        | ~45% (most frequent)                         |\n",
    "| **Arrays / Strings**      | ~35%                                         |\n",
    "| **Linked Lists**          | ~10%                                         |\n",
    "| **Searching / Sorting**   | ~5% (combined)                               |\n",
    "| **Stacks & Queues**       | ~2%                                          |\n",
    "| **Hash Tables**           | ~2%                                          |\n",
    "\n",
    "This guide covers the core patterns and techniques needed to solve problems in these areas. By mastering these patterns, you can map new problems to known solutions ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=Every%20software%20engineer%20should%20learn,interview%20preparation%20a%20streamlined%20process)) ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=1,of%20the%20Two%20Pointers%20pattern)) and tackle them confidently.\n",
    "\n",
    "## 1. Two Pointers Pattern\n",
    "\n",
    "**Description:** The two pointers technique uses two indices to iterate through a data structure (usually an array or string) from either the same end or opposite ends. It is extremely common for array and string problems ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=During%20my%20Amazon%20interview%20preparation%2C,Some%20common%20problems%20include)) ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=1,of%20the%20Two%20Pointers%20pattern)). Amazon frequently asks array/string questions that use two-pointer approaches (e.g. pair sums, partitioning, palindrome checks) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=During%20my%20Amazon%20interview%20preparation%2C,Some%20common%20problems%20include)).\n",
    "\n",
    "**When to Use:** Use two pointers when you need to find pairs or segments in sorted arrays, or when you can maintain a left and right boundary for a window or subarray. It’s useful for **avoiding nested loops** by moving two indices towards each other or one after the other.\n",
    "\n",
    "**Variations:**\n",
    "- **Opposite Ends:** One pointer starts at the beginning and one at the end, moving inward. Use this for sorted array pair-sum problems (finding two numbers that add to target), partitioning around a pivot, or checking palindromes.\n",
    "- **Fast and Slow (Tortoise and Hare):** One pointer moves one step at a time while the other moves two steps. This is common in linked list problems (to find the middle or detect cycles) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=Linked%20list%20problems%20test%20your,Be%20ready%20for)), and in some array problems (e.g. finding duplicates in a cycle of numbers).\n",
    "\n",
    "**Common Examples:** \n",
    "- *Pair Sum in Sorted Array:* Use one pointer at start and one at end to find a pair with given sum. **Example:** [Two Sum II – Input Array Is Sorted](https://leetcode.com/problems/two-sum-ii-input-array-is-sorted/) (LC 167).  \n",
    "- *Removing Duplicates:* Maintain a “slow” pointer for the place to insert next unique element. **Example:** [Remove Duplicates from Sorted Array](https://leetcode.com/problems/remove-duplicates-from-sorted-array/) (LC 26).  \n",
    "- *Linked List Cycle Detection:* Use fast/slow pointers to detect a cycle in a linked list (Floyd’s cycle detection). **Example:** [Linked List Cycle](https://leetcode.com/problems/linked-list-cycle/) (LC 141).  \n",
    "- *Reorder/Merge Linked List:* Use slow/fast to find middle, then rearrange pointers. **Examples:** [Palindrome Linked List](https://leetcode.com/problems/palindrome-linked-list/) (LC 234) – find midpoint and check palindrome, [Merge Two Sorted Lists](https://leetcode.com/problems/merge-two-sorted-lists/) (LC 21) – although typically done with one pointer on each list alternately.\n",
    "\n",
    "**Why it’s important:** Two pointers cover a huge set of array/string questions ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=1,of%20the%20Two%20Pointers%20pattern)), eliminating the need for O(n²) solutions by using simultaneous traversals. Amazon often starts with or includes such problems to test logical thinking and optimization skills ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=During%20my%20Amazon%20interview%20preparation%2C,Some%20common%20problems%20include)).\n",
    "\n",
    "## 2. Sliding Window Pattern\n",
    "\n",
    "**Description:** The sliding window pattern is used for problems on contiguous subsequences of arrays or strings (subarrays or substrings). It involves maintaining a window (defined by two pointers or indices) that “slides” through the data structure, expanding or contracting to satisfy a condition. This pattern is commonly used with strings and arrays, often alongside hash tables for tracking counts ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=2,like%20Arrays%2C%20Strings%2C%20and%20HashTables)).\n",
    "\n",
    "**When to Use:** Apply sliding window when you need to find an optimal subarray/substring (longest, shortest, or with certain property) or when you are dealing with contiguous segments. It’s effective for problems involving sums, averages, or unique elements in a subarray, where using a running calculation is possible.\n",
    "\n",
    "**Variations:**\n",
    "- **Fixed-size window:** When the window size is known or fixed. Slide the window by moving both pointers together. *Example:* finding max sum of any subarray of size *k*.\n",
    "- **Variable-size window:** When the window size is dynamic and depends on conditions (e.g., sum or unique characters). Typically one pointer expands the window, and the other shrinks it when a condition is violated (two-pointer technique in tandem).\n",
    "\n",
    "**Common Examples:** \n",
    "- *Longest Substring Without Repeating Characters:* Expand the window with one pointer, and move the start pointer when a repeat is found, using a hash set or map to track characters ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=%2A%20Two,maximum%20sum%20of%20a%20subarray)). **Example:** [Longest Substring Without Repeating Characters](https://leetcode.com/problems/longest-substring-without-repeating-characters/) (LC 3).  \n",
    "- *Max/Min Sum Subarray of Length K:* Use a fixed window of size *k*, slide it across the array while updating the sum. **Example:** [Maximum Average Subarray I](https://leetcode.com/problems/maximum-average-subarray-i/) (LC 643) – similar concept for sum.  \n",
    "- *Smallest Subarray with Given Sum:* Expand to accumulate sum and contract to remove excess until the window is minimal for the target sum. **Example:** [Minimum Size Subarray Sum](https://leetcode.com/problems/minimum-size-subarray-sum/) (LC 209).  \n",
    "- *Find All Anagrams in a String:* Use a window the size of the pattern, slide through the text string and use a frequency map to check matches. **Example:** [Find All Anagrams in a String](https://leetcode.com/problems/find-all-anagrams-in-a-string/) (LC 438).\n",
    "\n",
    "**Why it’s important:** Sliding window efficiently handles many string and array problems by reducing brute-force complexity. Many Amazon string/array questions (substrings, subarrays, etc.) are best solved with this pattern ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=%2A%20Two,sorting%20algorithms%2C%20binary%20search)). It leverages the idea of maintaining state as the window moves, which is a common interview theme.\n",
    "\n",
    "## 3. Hash Tables and Frequency Counting\n",
    "\n",
    "**Description:** Hash tables (hash maps and hash sets) provide O(1) average lookups and are used to store and retrieve data efficiently by keys. In interview problems, they often help in checking membership, counting frequencies, or caching results. Amazon uses hashing problems to test how candidates handle data efficiently ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=Amazon%20uses%20hashing%20problems%20to,sets%20are%20key%20tools%20here)).\n",
    "\n",
    "**When to Use:** Use hash maps/sets when you need to count occurrences, track seen elements, or map keys to values for quick lookup. They are ideal for problems involving complements (like two-sum), frequency counts (anagrams, substrings), or caching computations (dynamic programming memoization).\n",
    "\n",
    "**Variations:**\n",
    "- **Hash Set for existence:** Check if an element has been seen before. E.g., detecting duplicates or cycles.\n",
    "- **Hash Map for counts or mapping:** Count frequencies of elements (characters, numbers) or map an input to a desired output (e.g., mapping array values to indices, or storing computed results for subproblems).\n",
    "\n",
    "**Common Examples:** \n",
    "- *Two Sum:* Use a hash map to store values -> index, and for each number check if the complement (target - num) exists (O(n) solution). **Example:** [Two Sum](https://leetcode.com/problems/two-sum/) (LC 1). This classic problem tests basic map usage and is very common in interviews. \n",
    "- *Group Anagrams:* Use a hash map where the key is a sorted version of the string (or a frequency signature) and the value is a list of strings with those counts. **Example:** [Group Anagrams](https://leetcode.com/problems/group-anagrams/) (LC 49). This tests ability to use hashing for categorization.  \n",
    "- *Top K Frequent Elements:* Use a hash map to count frequencies, then use a heap or sort to find the most frequent. **Example:** [Top K Frequent Elements](https://leetcode.com/problems/top-k-frequent-elements/) (LC 347). (Combines hashing with another structure).  \n",
    "- *Subarray Sum Equals K:* Use a hash map to store prefix sum frequencies. For each new prefix sum, check if `prefix_sum - K` was seen (indicating a subarray summing to K). **Example:** [Subarray Sum Equals K](https://leetcode.com/problems/subarray-sum-equals-k/) (LC 560).\n",
    "\n",
    "**Why it’s important:** Hashing is a fundamental technique to optimize brute-force solutions. Amazon often includes problems that are trivial with a hash map but hard without (e.g., two-sum) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=Amazon%20uses%20hashing%20problems%20to,sets%20are%20key%20tools%20here)). Knowing how to use hash tables in tandem with other patterns (sliding window + hash map for counts, two-pointer + hash set for seen elements, etc.) is key to solving many interview questions efficiently.\n",
    "\n",
    "## 4. Binary Search\n",
    "\n",
    "**Description:** Binary search is an algorithmic technique to find an element in a **sorted** array (or search space) in O(log n) time by repeatedly dividing the range in half. Beyond searching a sorted list, it can be applied to problems where you need to find an optimal value in a monotonic search space (binary search on answer). Sorting and searching are fundamental skills expected of Amazon candidates ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=%2A%20Two,sorting%20algorithms%2C%20binary%20search)).\n",
    "\n",
    "**When to Use:** Use binary search when the input array is sorted or when you can formulate the problem in terms of a sorted sequence or monotonic function. Typical scenarios:\n",
    "- Finding a target value or boundary in a sorted array.\n",
    "- Finding a condition change in a boolean monotonic array (first true/false).\n",
    "- Searching for a numeric answer that satisfies a condition (e.g., minimize max load, etc.).\n",
    "\n",
    "**Variations:**\n",
    "- **Basic Binary Search:** Check mid element, narrow down to left or right half. Use for direct lookups or insert positions.\n",
    "- **Binary Search for Boundaries:** When looking for first or last occurrence of a value (lower_bound/upper_bound logic) – requires careful handling of mid and boundaries.\n",
    "- **Binary Search on Answer:** When the solution is not an index but a number that can be evaluated with a yes/no condition (e.g., find smallest capacity to ship packages in D days, where a feasible check is monotonic in capacity).\n",
    "\n",
    "**Common Examples:** \n",
    "- *Search in Sorted Array:* Classic binary search for a value. **Example:** [Binary Search](https://leetcode.com/problems/binary-search/) (LC 704).  \n",
    "- *First and Last Position:* Find the first and last occurrence of a target in a sorted array (requires finding boundaries via binary search). **Example:** [Find First and Last Position of Element](https://leetcode.com/problems/find-first-and-last-position-of-element-in-sorted-array/) (LC 34).  \n",
    "- *Search in Rotated Sorted Array:* The array is sorted but rotated; binary search with an extra check can find the target in O(log n). **Example:** [Search in Rotated Sorted Array](https://leetcode.com/problems/search-in-rotated-sorted-array/) (LC 33).  \n",
    "- *2D Matrix Search:* Treat a sorted matrix as a flat sorted list or do a two-phase binary search (first on rows, then on column). **Example:** [Search a 2D Matrix](https://leetcode.com/problems/search-a-2d-matrix/) (LC 74) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=,2D%20matrix%2C%20rotating%20a%20matrix)).  \n",
    "- *Binary Search on Answer:* E.g., [Capacity To Ship Packages in D Days](https://leetcode.com/problems/capacity-to-ship-packages-within-d-days/) (LC 1011) – search for the minimum capacity that works by testing mid capacity.\n",
    "\n",
    "**Why it’s important:** Binary search and sorting underpin many other algorithms. Even if a question isn’t explicitly “perform binary search,” recognizing a sorted input or monotonic condition and applying binary search is a mark of a strong problem-solver. Amazon expects you to be comfortable with log-time solutions when applicable (e.g., searching large sorted data) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=%2A%20Two,sorting%20algorithms%2C%20binary%20search)).\n",
    "\n",
    "## 5. Linked List Manipulation\n",
    "\n",
    "**Description:** Linked list problems focus on pointer manipulation in a sequence of nodes. Common tasks include reversing a list, merging lists, finding cycles, or removing nodes. These questions test understanding of references/pointers and edge-case handling (nulls, list ends). Amazon likes to include at least one linked list problem to gauge your comfort with pointer logic ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=3)).\n",
    "\n",
    "**When to Use:** Recognize linked list patterns when dealing with lists of nodes where random access is not possible (unlike arrays). Typical problems involve rearranging nodes, detecting loops, or performing arithmetic on linked list representations of numbers.\n",
    "\n",
    "**Variations:**\n",
    "- **Reversal:** Reverse the entire list or parts of it (in-place pointer manipulation). Variants include reversing the whole list, or reversing sub-parts (k-group reversal).\n",
    "- **Cycle Detection:** Using fast/slow two-pointer technique to detect if a cycle exists, and possibly find the start of the cycle.\n",
    "- **Merging and Sorting:** Merging two sorted lists into one (two-pointer technique on two lists), or merging k lists (often using a min-heap or divide-and-conquer). Also, partitioning a list around a value (like quicksort partition logic).\n",
    "- **Removal and Retrieval:** Removing nth node from end (two pointers spaced n apart), finding middle node, etc.\n",
    "\n",
    "**Common Examples:** \n",
    "- *Reverse a Linked List:* Iteratively or recursively reverse pointers. **Example:** [Reverse Linked List](https://leetcode.com/problems/reverse-linked-list/) (LC 206).  \n",
    "- *Detect Cycle in Linked List:* Use fast and slow pointers to detect a loop. **Example:** [Linked List Cycle](https://leetcode.com/problems/linked-list-cycle/) (LC 141). (Follow-up: find the cycle start using math once detected.)  \n",
    "- *Merge Two Sorted Lists:* Iteratively compare heads of two lists and build a sorted result. **Example:** [Merge Two Sorted Lists](https://leetcode.com/problems/merge-two-sorted-lists/) (LC 21). (A basic merge, often asked as an easy warm-up).  \n",
    "- *Merge K Sorted Lists:* Use a min-heap (priority queue) to always take the smallest head among k lists, or recursively merge pairs of lists. **Example:** [Merge k Sorted Lists](https://leetcode.com/problems/merge-k-sorted-lists/) (LC 23).  \n",
    "- *Remove Nth Node from End:* Use two pointers spaced n apart. **Example:** [Remove Nth Node From End of List](https://leetcode.com/problems/remove-nth-node-from-end-of-list/) (LC 19).\n",
    "\n",
    "**Why it’s important:** Linked list problems test low-level understanding of memory references and pointer handling, which is important for writing correct and efficient code. Amazon interviewers often include these to ensure you can handle null pointers, edge cases (single node, even/odd length, etc.), and linear data structures manipulation ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=Linked%20list%20problems%20test%20your,Be%20ready%20for)).\n",
    "\n",
    "## 6. Breadth-First Search (BFS) on Trees/Graphs\n",
    "\n",
    "**Description:** BFS is a traversal technique that explores nodes level by level (breadth-wise) using a queue. In trees, BFS visits nodes level-order from the root. In graphs, BFS finds all neighbors of a node before moving to the next level. BFS is especially useful for finding the **shortest path in unweighted graphs** or any scenario requiring the minimal number of steps, as well as for level-order traversal of trees ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=4,a%20good%20number%20of%20problems)).\n",
    "\n",
    "**When to Use:** Use BFS when the problem asks for the shortest path, minimum number of moves, or anything that inherently spreads outwards from a source. In tree problems, use BFS for level-order traversal or when you need to process nodes layer by layer (e.g., connect all nodes at same depth, etc.). BFS is also used in graph problems where you need to find if something is reachable in *k* steps or the shortest distance.\n",
    "\n",
    "**Variations:**\n",
    "- **Standard BFS:** Use a queue to traverse. For graphs, mark visited nodes to avoid repetition. For trees, just use the queue since no cycles.\n",
    "- **BFS with Depth Tracking:** Sometimes you need to know the level (depth) of each node (e.g., shortest path length, or zigzag level order). This can be done by level-order traversal (queue size or sentinel to separate levels).\n",
    "- **Multi-source BFS:** Start BFS from multiple start nodes simultaneously. Useful in problems like “rotting oranges” where all initial rotten oranges spread in parallel.\n",
    "- **BFS for Topological Sort:** Kahn’s algorithm uses BFS on DAGs by starting from nodes with in-degree 0 and removing them layer by layer (resolving prerequisites in order).\n",
    "\n",
    "**Common Examples:** \n",
    "- *Level Order Traversal (Tree):* Traverse a binary tree by levels. **Example:** [Binary Tree Level Order Traversal](https://leetcode.com/problems/binary-tree-level-order-traversal/) (LC 102). This is a classic BFS on a tree.  \n",
    "- *Minimum Depth of Binary Tree:* Find the shortest path to a leaf node (first leaf encountered via BFS gives min depth). **Example:** [Minimum Depth of Binary Tree](https://leetcode.com/problems/minimum-depth-of-binary-tree/) (LC 111).  \n",
    "- *Shortest Path in Grid/Matrix:* Treat the grid as a graph, BFS from the start to find the shortest path to a target. **Example:** [Shortest Path in Binary Matrix](https://leetcode.com/problems/shortest-path-in-binary-matrix/) (LC 1091). Similarly, the classic **Maze problem** or **Knight’s move** problem use BFS.  \n",
    "- *Number of Islands (using BFS):* Count connected components in a grid by BFS (flip 1s to 0s as you traverse). **Example:** [Number of Islands](https://leetcode.com/problems/number-of-islands/) (LC 200). (Can also be done with DFS—either is fine.)  \n",
    "- *Course Schedule (Topological Sort):* Determine if you can finish all courses given prerequisite pairs. Use BFS by first taking courses with no prerequisites. **Example:** [Course Schedule](https://leetcode.com/problems/course-schedule/) (LC 207). (Build graph and use BFS to detect if all nodes can be visited – no cycle).  \n",
    "- *Word Ladder:* Each word is a node, and edges connect words that differ by one letter. BFS finds the shortest transformation sequence. **Example:** [Word Ladder](https://leetcode.com/problems/word-ladder/) (LC 127). This is a classic shortest-path BFS in an implicit graph of words.\n",
    "\n",
    "**Why it’s important:** A significant portion of Amazon questions involve tree or graph traversals ([Amazon Software Development Engineer Interview (questions, process, prep) - IGotAnOffer](https://igotanoffer.com/blogs/tech/amazon-software-development-engineer-interview#:~:text=Here%20are%20the%20most%20common,cover%20later%20in%20this%20article)). BFS is a core technique for those, especially for shortest-path problems or any level-by-level processing. Many problems (like scheduling tasks, navigating grids, or connected components) are intuitively solved via BFS. Mastering BFS helps you solve a variety of questions systematically, ensuring you consider optimal paths and layer-by-layer exploration.\n",
    "\n",
    "## 7. Depth-First Search (DFS) on Trees/Graphs\n",
    "\n",
    "**Description:** DFS is a traversal that goes as deep as possible down one path before backtracking. In trees, DFS is implemented via recursion or a stack (preorder, inorder, postorder traversals). In graphs, DFS can explore components and detect cycles. It’s useful for exhaustive search of solution spaces and is the basis for backtracking algorithms. **Most tree and graph problems can be solved using DFS or its variants** ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=3,related%20problems)).\n",
    "\n",
    "**When to Use:** Use DFS when you need to explore all possibilities (combinatorial search), or when implementing algorithms that naturally use recursion (tree traversals, divide-and-conquer). In graphs, use DFS to find connected components, detect cycles, or do topological sort (DFS-based). For puzzles and games (like solving a maze, word search), DFS (possibly with backtracking) is a natural choice to explore all possible moves.\n",
    "\n",
    "**Variations:**\n",
    "- **Tree DFS (Recursion):** Traverse a tree in Preorder, Inorder, or Postorder depending on the problem. Many tree problems (e.g., computing sums, depths, validating BST) use DFS recursion.\n",
    "- **Graph DFS:** Use a stack or recursion to explore graph nodes. Mark visited nodes to avoid infinite loops. Good for connectivity and cycle detection.\n",
    "- **Backtracking:** A special DFS that explores all possible solutions (see next section) by trying options and undoing (backtracking) them.\n",
    "- **Recursive Divide-and-Conquer:** Some problems (like binary tree algorithms for diameter, max path sum, etc.) use DFS to compute results from children up to parent.\n",
    "\n",
    "**Common Examples:** \n",
    "- *Tree Traversals:* **Example:** [Binary Tree Inorder Traversal](https://leetcode.com/problems/binary-tree-inorder-traversal/) (LC 94) – classic recursion or stack use. Preorder and postorder are similar.  \n",
    "- *Max Depth or Path Sum of a Tree:* DFS to compute depth or accumulate sums. **Examples:** [Maximum Depth of Binary Tree](https://leetcode.com/problems/maximum-depth-of-binary-tree/) (LC 104), [Path Sum](https://leetcode.com/problems/path-sum/) (LC 112).  \n",
    "- *Validate Binary Search Tree:* Inorder DFS traversal to ensure sorted order of values. **Example:** [Validate BST](https://leetcode.com/problems/validate-binary-search-tree/) (LC 98).  \n",
    "- *Graph Connectivity:* Count components or check if a graph is connected. **Example:** [Number of Connected Components in an Undirected Graph](https://leetcode.com/problems/number-of-connected-components-in-an-undirected-graph/) (LC 323). (Similar to number of islands but on an arbitrary graph.)  \n",
    "- *Detect Cycle in Graph:* Use DFS (or Union-Find) to detect back-edges. **Example:** [Course Schedule II](https://leetcode.com/problems/course-schedule-ii/) (LC 210) – uses DFS to detect cycle (if a cycle is found, no topological ordering).  \n",
    "- *Clone Graph:* DFS through graph and clone nodes. **Example:** [Clone Graph](https://leetcode.com/problems/clone-graph/) (LC 133).  \n",
    "- *Matrix DFS (Flood Fill):* **Example:** [Flood Fill](https://leetcode.com/problems/flood-fill/) (LC 733) – use DFS to paint connected area.  \n",
    "*(Note: many of the above can also be done with BFS; choose based on what’s easier to implement or what the question hints at.)*\n",
    "\n",
    "**Why it’s important:** DFS is a versatile tool—**it underlies backtracking, recursion, and many tree/graph algorithms**. Amazon interviews often involve tree recursion (for computing properties of trees) or graph DFS for exploring relationships (like courses, dependencies, islands). A solid understanding of DFS helps in reasoning through recursive solutions and is essential for the next category (backtracking). It also demonstrates your ability to traverse complex data structures systematically.\n",
    "\n",
    "## 8. Backtracking (Recursive Search)\n",
    "\n",
    "**Description:** Backtracking is a DFS-based technique for exploring **all possible solutions** by incrementally building candidates and abandoning (backtracking) when a candidate cannot lead to a valid solution. It is used for combinatorial problems (permutations, combinations, subsets) and constraint satisfaction (sudoku, n-queens). Essentially, it’s a trial-and-error search with pruning. Amazon occasionally asks backtracking problems, especially those that involve generating combinations or recursive brute-force with optimizations.\n",
    "\n",
    "**When to Use:** Use backtracking for problems where you need to generate **all solutions** or find a solution by trying options, such as:\n",
    "- Generating permutations or combinations of a set.\n",
    "- Solving puzzles (crosswords, sudoku, N-Queens).\n",
    "- Partitioning problems (like subset sum, if using DFS).\n",
    "- Whenever the problem says “print all sequences” or “find all combinations that satisfy X”.\n",
    "\n",
    "**Variations:**\n",
    "- **Permutation generation:** Choose an element, then recursively permute the rest. (Use swapping or track used elements.)\n",
    "- **Combination generation:** Make a choice to include or exclude an element and backtrack accordingly (subsets, combinations).\n",
    "- **Constraint satisfaction:** Place elements while checking partial validity (e.g., place queens one row at a time, only continue if safe).\n",
    "- **Optimization with Backtracking:** Use pruning (like bounding functions or sorting) to cut off branches (e.g., backtracking + greedy pruning).\n",
    "\n",
    "**Common Examples:** \n",
    "- *Permutations of an Array/String:* Generate all permutations by swapping or using a visited array. **Example:** [Permutations](https://leetcode.com/problems/permutations/) (LC 46).  \n",
    "- *Combinations / Subsets:* Generate all subsets (power set) or combinations of k elements. **Examples:** [Subsets](https://leetcode.com/problems/subsets/) (LC 78), [Combinations](https://leetcode.com/problems/combinations/) (LC 77).  \n",
    "- *Combination Sum:* Find all combinations of numbers that sum to a target (candidates can be reused or not, depending on version). **Example:** [Combination Sum](https://leetcode.com/problems/combination-sum/) (LC 39).  \n",
    "- *Generate Parentheses:* Backtrack by adding '(' or ')' as long as the sequence remains valid. **Example:** [Generate Parentheses](https://leetcode.com/problems/generate-parentheses/) (LC 22). (This is a classic Amazon question – generate all well-formed parentheses pairs).  \n",
    "- *N-Queens Problem:* Place N queens on an N×N board so that none attack each other. **Example:** [N-Queens](https://leetcode.com/problems/n-queens/) (LC 51). Use backtracking to place queens row by row and backtrack when a placement is not safe.  \n",
    "- *Word Search:* DFS/backtracking on a grid to find if a word exists by exploring all paths. **Example:** [Word Search](https://leetcode.com/problems/word-search/) (LC 79).\n",
    "\n",
    "**Why it’s important:** Backtracking problems test your ability to think recursively and systematically explore possibilities. They also test pruning – realizing when to stop exploring a path. While backtracking can be time-consuming, Amazon may include one to see if you can translate a problem description into a recursive exploration (and possibly optimize it). Mastering backtracking also strengthens your understanding of DFS and recursion, which is valuable in many dynamic programming problems as well.\n",
    "\n",
    "## 9. Dynamic Programming (DP)\n",
    "\n",
    "**Description:** Dynamic Programming is an optimization technique to solve problems with **overlapping subproblems** and **optimal substructure**. It involves breaking a problem into subproblems, solving each subproblem once, and storing their results (often using a table or memoization) so as not to recompute them. DP is used for a wide range of optimization, counting, or pathfinding problems. At Amazon, DP problems are common and **test your ability to break down complex problems into smaller subproblems** (a key insight for many algorithmic challenges).\n",
    "\n",
    "**When to Use:** Use DP when the problem asks for an optimal result (max/min, longest/shortest, count of ways, etc.) and has overlapping sub-cases. Clues that suggest DP:\n",
    "- “Find the number of ways to do X” (counting paths, sequences, etc.).\n",
    "- “Find the maximum/minimum ...” subject to some constraints (knapsack-like, partitioning).\n",
    "- Problems that can be described recursively with results reused (Fibonacci, grid paths).\n",
    "- Strings comparison problems (edit distance, longest common subsequence).\n",
    "\n",
    "**Variations:**\n",
    "- **1-D DP:** A one-dimensional state (often an array indexed by one variable). E.g., Fibonacci sequence, climbing stairs (dp[i] depends on previous states), or 1D interval problems like house robber.\n",
    "- **2-D DP:** Two-dimensional state (indexed by two variables). Common in grid problems (indexed by row and column) or string alignment problems (indexed by indices of two strings, e.g., DP[i][j] for first i chars and first j chars).  \n",
    "- **DP with additional dimensions:** Less common for SDE I, but can include 3D DP for certain complex states or bitmask DP for subsets (like traveling salesman, but that’s advanced).\n",
    "- **Memoization vs Tabulation:** Top-down memoization (recursion + caching) vs bottom-up tabulation. Both achieve the same results; choose based on what’s easier to implement.\n",
    "\n",
    "**Common Examples:** \n",
    "- *Climbing Stairs / Fibonacci:* Simple DP where `dp[n] = dp[n-1] + dp[n-2]`. **Example:** [Climbing Stairs](https://leetcode.com/problems/climbing-stairs/) (LC 70). (A classic entry DP problem).  \n",
    "- *House Robber:* 1D DP on an array of house values: `dp[i] = max(dp[i-1], dp[i-2] + value[i])` (either rob this house and add to i-2, or skip it). **Example:** [House Robber](https://leetcode.com/problems/house-robber/) (LC 198). Variations include House Robber II (circular houses) and III (rob binary tree with no adjacent nodes).  \n",
    "- *Unique Paths in a Grid:* 2D DP where `dp[i][j] = dp[i-1][j] + dp[i][j-1]` (sum of ways from top or left). **Example:** [Unique Paths](https://leetcode.com/problems/unique-paths/) (LC 62).  \n",
    "- *Coin Change / Minimum Coins:* DP to compute the fewest coins needed for an amount (unbounded knapsack). **Example:** [Coin Change](https://leetcode.com/problems/coin-change/) (LC 322). Also, *Coin Change 2* (count ways to make amount) is a classic DP.  \n",
    "- *Longest Increasing Subsequence (LIS):* 1D DP where `dp[i]` is the length of longest increasing subsequence ending at index i (compare with all j < i). **Example:** [Longest Increasing Subsequence](https://leetcode.com/problems/longest-increasing-subsequence/) (LC 300).  \n",
    "- *Longest Common Subsequence (LCS):* 2D DP on two strings (i and j indices for each string). **Example:** [Longest Common Subsequence](https://leetcode.com/problems/longest-common-subsequence/) (LC 1143). Similarly, *Edit Distance* (Levenshtein distance) is a 2D DP on two strings. **Example:** [Edit Distance](https://leetcode.com/problems/edit-distance/) (LC 72).  \n",
    "- *Partition Equal Subset Sum:* Subset-sum DP (knapsack-like) to determine if a subset sums to target. **Example:** [Partition Equal Subset Sum](https://leetcode.com/problems/partition-equal-subset-sum/) (LC 416).  \n",
    "- *Decode Ways:* Count ways to decode a numeric string (like mapping to letters). DP[i] depends on previous one or two states if valid. **Example:** [Decode Ways](https://leetcode.com/problems/decode-ways/) (LC 91) – a frequently discussed DP problem (ways to decode a message).  \n",
    "\n",
    "**Why it’s important:** Dynamic programming problems are common in interviews because they test your ability to identify subproblem structure and use additional memory to optimize runtime. **Amazon does ask DP questions** (e.g., partition problems, path counting, etc.), and being able to formulate a DP solution demonstrates strong problem-solving skills. When solving DP, clearly communicate your recurrence relation and state definition in an interview – this shows structured thinking. Practice classic DP problems so you recognize patterns (Fibonacci-style, knapsack-style, sequence alignment, etc.) and can adapt them to new questions.\n",
    "\n",
    "## 10. Greedy Algorithms and Interval Scheduling\n",
    "\n",
    "**Description:** Greedy algorithms involve making the locally optimal choice at each step, hoping to find a global optimum. They are often simpler and more efficient than DP for the right problems, but they work only when a *greedy choice property* holds (choosing local optimum leads to global optimum). Many interval scheduling and optimization problems use greedy strategies. Amazon interview questions sometimes have elegant greedy solutions (e.g., interval merging, task scheduling, resource allocation problems).\n",
    "\n",
    "**When to Use:** Consider a greedy approach when:\n",
    "- Sorting the data reveals a simple strategy (e.g., sort intervals by start time and then merge).\n",
    "- You need to minimize or maximize some quantity and making a locally optimal choice (like taking the shortest job first, earliest finishing interval first, etc.) seems to work.\n",
    "- The problem asks for an optimal result and you can argue why a DP might be overkill (if the greedy choice can be proven optimal).\n",
    "\n",
    "**Variations:**\n",
    "- **Interval Scheduling/Merging:** Sort intervals by start or end times. For scheduling, choose the interval that finishes first (to accommodate more intervals) ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=6,frequently%20appear%20in%20coding%20interviews)). For merging, sort by start and merge overlapping ones.\n",
    "- **Selection problems:** At each step, pick the largest or smallest item that meets criteria (e.g., always take the highest value remaining, or the shortest distance first).\n",
    "- **Greedy with Heap:** Use a priority queue to always choose the next best element (e.g., always extract the largest remaining task, or the next meeting to end).\n",
    "- **Greedy with Sorting:** Sort data and then iterate, using greedy decisions (like partitioning labels problem, or distributing resources in order).\n",
    "\n",
    "**Common Examples:** \n",
    "- *Merge Intervals:* Sort intervals by start time, then iterate and merge overlapping intervals greedily. **Example:** [Merge Intervals](https://leetcode.com/problems/merge-intervals/) (LC 56).  \n",
    "- *Non-overlapping Intervals (Interval Scheduling):* Choose the maximum number of non-conflicting intervals by always picking the next interval that ends earliest. **Example:** [Non-overlapping Intervals](https://leetcode.com/problems/non-overlapping-intervals/) (LC 435). (Or dually, minimize removals to avoid overlap).  \n",
    "- *Meeting Rooms (Minimum Number of Rooms):* To find how many meetings overlap, sort start and end times and use a min-heap or two-pointer sweep line. **Example:** [Meeting Rooms II](https://leetcode.com/problems/meeting-rooms-ii/) (LC 253). Greedy: allocate room as needed, re-use a room when an earlier meeting ends (min-heap of end times).  \n",
    "- *Jump Game:* Determine if you can reach the end of array by jumping. Greedy solution: track the furthest reachable index and iterate through array updating this reach ([Common Amazon Coding Interview Questions](https://www.interviewhelp.io/blog/posts/common-amazon-coding-interview-questions/#:~:text=,0th%20index%20of%20the%20array)). **Example:** [Jump Game](https://leetcode.com/problems/jump-game/) (LC 55). (At Amazon, this is commonly asked – can be solved greedily by always jumping to the best next reachable index).  \n",
    "- *Jump Game II:* Minimize jumps to reach the end. Greedy: within the current range of reach, determine how far we can get in the next jump. **Example:** [Jump Game II](https://leetcode.com/problems/jump-game-ii/) (LC 45).  \n",
    "- *Gas Station Circuit:* Given gas and cost at stations in a circle, find start station to complete circuit. Greedy solution: if you can't reach a station, start after it; there is a known greedy proof for this. **Example:** [Gas Station](https://leetcode.com/problems/gas-station/) (LC 134) ([Common Amazon Coding Interview Questions](https://www.interviewhelp.io/blog/posts/common-amazon-coding-interview-questions/#:~:text=,and%20B%20of%20size%20N)).  \n",
    "- *Partition Labels:* Split string into largest chunks such that each letter appears in only one chunk. Greedy by tracking last occurrence of each char and cutting when the current index reaches the max last occurrence seen so far. **Example:** [Partition Labels](https://leetcode.com/problems/partition-labels/) (LC 763).  \n",
    "- *Task Scheduler:* Given tasks with cooldown, schedule tasks greedily by always executing the task with the highest remaining count (use max-heap) and idling when needed. **Example:** [Task Scheduler](https://leetcode.com/problems/task-scheduler/) (LC 621). (Greedy + counting frequency).  \n",
    "- *Kth Largest / Smallest:* While you can sort, using a min-heap or max-heap is greedy-like for partial selection. **Examples:** [Kth Largest Element in an Array](https://leetcode.com/problems/kth-largest-element-in-an-array/) (LC 215) – use a heap or Quickselect (Quickselect is another partial greedy approach using partitioning).\n",
    "\n",
    "**Why it’s important:** Greedy solutions are elegant and efficient; a good candidate should recognize when a greedy approach works. Amazon may present a problem that could be solved via DP or brute force, but a greedy insight drastically simplifies it – expecting you to spot the greedy strategy. Interval problems (meeting schedules, merges) are particularly common and always solved greedily ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=6,frequently%20appear%20in%20coding%20interviews)). Knowing classic greedy algorithms (interval scheduling, Huffman coding, Dijkstra for weighted shortest path, etc.) can help, but for SDE I, focus on the patterns above. When discussing a greedy solution, be prepared to justify *why* the greedy choice leads to an optimal solution (this shows depth of understanding).\n",
    "\n",
    "## 11. Additional Patterns and Techniques\n",
    "\n",
    "Finally, here are a few other patterns that are less common at Amazon SDE I interviews but still worth noting if time permits:\n",
    "\n",
    "- **Stack-Based Patterns:** Some problems use a stack to maintain a certain structure:\n",
    "  - *Balanced Parentheses:* Use a stack to validate strings of brackets. **Example:** [Valid Parentheses](https://leetcode.com/problems/valid-parentheses/) (LC 20). (Classic easy question testing stack usage).  \n",
    "  - *Monotonic Stack:* Used for next greater/smaller element or stock span problems. **Example:** [Next Greater Element](https://leetcode.com/problems/next-greater-element-i/) (LC 496) and [Daily Temperatures](https://leetcode.com/problems/daily-temperatures/) (LC 739). These appear less frequently, but understanding the idea of a stack that maintains a monotonic order is useful for certain array problems (like calculating areas in a histogram, etc.).  \n",
    "  - *Min Stack / Implement Queue using Stacks:* These are design questions using stack properties, sometimes asked as quick problem-solving checks.\n",
    "\n",
    "- **Queue-Based Patterns:** Apart from BFS, queue can be used in:\n",
    "  - *Sliding Window Maximum:* Uses a deque (double-ended queue) to maintain the max in the current window efficiently by popping smaller values from back. **Example:** [Sliding Window Maximum](https://leetcode.com/problems/sliding-window-maximum/) (LC 239). (This is an optimization of the sliding window pattern – more advanced, but noted for completeness.)\n",
    "\n",
    "- **Union-Find (Disjoint Set Union):** A data structure for tracking components, useful in some graph problems (connecting networks, Kruskal’s MST, cycle detection in undirected graphs). It’s not commonly the focus in Amazon interviews (more so in Google/Facebook), but it appears occasionally:\n",
    "  - *Find if Graph is Tree:* Use union-find to check for cycle and connectivity. **Example:** [Graph Valid Tree](https://leetcode.com/problems/graph-valid-tree/) (LC 261).  \n",
    "  - *Accounts Merge:* Union-find to group email accounts. **Example:** [Accounts Merge](https://leetcode.com/problems/accounts-merge/) (LC 721).\n",
    "\n",
    "- **Bit Manipulation:** Bit tricks are not usually a main topic at Amazon, but simple ones could appear:\n",
    "  - *Power of Two Check, Counting Bits, Single Number (XOR trick)* are classic bit problems. **Example:** [Single Number](https://leetcode.com/problems/single-number/) (LC 136) – uses XOR to cancel out pairs.  \n",
    "  - These are generally rare; focus on them only after mastering the above patterns.\n",
    "\n",
    "---\n",
    "\n",
    "**Preparation Tip:** Focus on the patterns above in roughly the priority order given. Practice a few representative LeetCode problems for each pattern to solidify the technique. By covering Two Pointers, Sliding Window, BFS/DFS, Backtracking, DP, Greedy, etc., you will have a toolkit capable of solving most Amazon interview questions. Remember, it’s not about memorizing solutions, but about recognizing which pattern a new problem maps to – *“mapping a new problem to an existing one”* ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=Every%20software%20engineer%20should%20learn,interview%20preparation%20a%20streamlined%20process)) is the key skill. Good luck with your interview preparation! \n",
    "\n",
    "**Sources:** Leveraged insights from interview guides and problem distributions ([Amazon Software Development Engineer Interview (questions, process, prep) - IGotAnOffer](https://igotanoffer.com/blogs/tech/amazon-software-development-engineer-interview#:~:text=1,of%20questions%2C%20least%20frequent)) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=During%20my%20Amazon%20interview%20preparation%2C,Some%20common%20problems%20include)) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=Amazon%20uses%20hashing%20problems%20to,sets%20are%20key%20tools%20here)) ([Amazon SDE coding assessment questions | Important problem types asked in amazon interviews | by Sahil Ali | Medium](https://sahilali.medium.com/amazon-sde-coding-assessment-questions-important-problem-types-asked-in-amazon-interviews-6a50ae89882a#:~:text=Linked%20list%20problems%20test%20your,Be%20ready%20for)) ([Top LeetCode Patterns to Crack FAANG Coding Interviews](https://www.designgurus.io/blog/top-lc-patterns#:~:text=3,related%20problems)) to prioritize topics and from classic LeetCode problems for examples. By studying these patterns and practicing their variations, you can approach Amazon’s coding interview with confidence and a structured problem-solving approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📅 Amazon Medium Coding Interview Questions – With LeetCode Links & Python Solutions\n",
    "\n",
    "---\n",
    "\n",
    "This notebook includes medium-difficulty problems frequently asked in Amazon interviews. Each section includes a LeetCode link, an optimal Python solution, and a detailed explanation.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Arrays and Strings\n",
    "\n",
    "### 🔗 [Group Anagrams](https://leetcode.com/problems/group-anagrams/)\n",
    "```python\n",
    "from collections import defaultdict\n",
    "\n",
    "def groupAnagrams(strs):\n",
    "    anagram_map = defaultdict(list)\n",
    "    for s in strs:\n",
    "        key = tuple(sorted(s))\n",
    "        anagram_map[key].append(s)\n",
    "    return list(anagram_map.values())\n",
    "```\n",
    "**Explanation**: Sort each word to form the key. Anagrams will share the same key in a hashmap.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [Product of Array Except Self](https://leetcode.com/problems/product-of-array-except-self/)\n",
    "```python\n",
    "def productExceptSelf(nums):\n",
    "    n = len(nums)\n",
    "    result = [1] * n\n",
    "\n",
    "    prefix = 1\n",
    "    for i in range(n):\n",
    "        result[i] = prefix\n",
    "        prefix *= nums[i]\n",
    "\n",
    "    suffix = 1\n",
    "    for i in range(n-1, -1, -1):\n",
    "        result[i] *= suffix\n",
    "        suffix *= nums[i]\n",
    "\n",
    "    return result\n",
    "```\n",
    "**Explanation**: Use two passes — one from the left to collect prefix products, and one from the right for suffix products. Multiply them.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Hash Maps and Sets\n",
    "\n",
    "### 🔗 [Subarray Sum Equals K](https://leetcode.com/problems/subarray-sum-equals-k/)\n",
    "```python\n",
    "def subarraySum(nums, k):\n",
    "    count = 0\n",
    "    cumulative_sum = 0\n",
    "    prefix_counts = {0: 1}\n",
    "\n",
    "    for num in nums:\n",
    "        cumulative_sum += num\n",
    "        if cumulative_sum - k in prefix_counts:\n",
    "            count += prefix_counts[cumulative_sum - k]\n",
    "        prefix_counts[cumulative_sum] = prefix_counts.get(cumulative_sum, 0) + 1\n",
    "    return count\n",
    "```\n",
    "**Explanation**: Track prefix sums and count how often `sum-k` has appeared to find valid subarrays.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [Top K Frequent Elements](https://leetcode.com/problems/top-k-frequent-elements/)\n",
    "```python\n",
    "import heapq\n",
    "from collections import Counter\n",
    "\n",
    "def topKFrequent(nums, k):\n",
    "    freq = Counter(nums)\n",
    "    return [item for item, count in heapq.nlargest(k, freq.items(), key=lambda x: x[1])]\n",
    "```\n",
    "**Explanation**: Count frequencies and retrieve the top k using a heap.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Two Pointers and Sliding Window\n",
    "\n",
    "### 🔗 [Longest Substring Without Repeating Characters](https://leetcode.com/problems/longest-substring-without-repeating-characters/)\n",
    "```python\n",
    "def lengthOfLongestSubstring(s):\n",
    "    last_seen = {}\n",
    "    max_length = 0\n",
    "    start = 0\n",
    "\n",
    "    for end, ch in enumerate(s):\n",
    "        if ch in last_seen and last_seen[ch] >= start:\n",
    "            start = last_seen[ch] + 1\n",
    "        last_seen[ch] = end\n",
    "        max_length = max(max_length, end - start + 1)\n",
    "    return max_length\n",
    "```\n",
    "**Explanation**: Use a sliding window and update the start whenever a repeat is found inside the window.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [3Sum](https://leetcode.com/problems/3sum/)\n",
    "```python\n",
    "def threeSum(nums):\n",
    "    nums.sort()\n",
    "    n = len(nums)\n",
    "    result = []\n",
    "\n",
    "    for i in range(n-2):\n",
    "        if i > 0 and nums[i] == nums[i-1]:\n",
    "            continue\n",
    "        target = -nums[i]\n",
    "        left, right = i+1, n-1\n",
    "\n",
    "        while left < right:\n",
    "            curr_sum = nums[left] + nums[right]\n",
    "            if curr_sum == target:\n",
    "                result.append([nums[i], nums[left], nums[right]])\n",
    "                while left < right and nums[left] == nums[left + 1]:\n",
    "                    left += 1\n",
    "                while left < right and nums[right] == nums[right - 1]:\n",
    "                    right -= 1\n",
    "                left += 1\n",
    "                right -= 1\n",
    "            elif curr_sum < target:\n",
    "                left += 1\n",
    "            else:\n",
    "                right -= 1\n",
    "    return result\n",
    "```\n",
    "**Explanation**: Sort the array and use two pointers for each fixed element to find valid triplets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Trees and Binary Trees\n",
    "\n",
    "### 🔗 [Binary Tree Zigzag Level Order Traversal](https://leetcode.com/problems/binary-tree-zigzag-level-order-traversal/)\n",
    "```python\n",
    "from collections import deque\n",
    "\n",
    "class TreeNode:\n",
    "    def __init__(self, val=0, left=None, right=None):\n",
    "        self.val = val\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "def zigzagLevelOrder(root):\n",
    "    if not root:\n",
    "        return []\n",
    "    result = []\n",
    "    queue = deque([root])\n",
    "    left_to_right = True\n",
    "\n",
    "    while queue:\n",
    "        level_size = len(queue)\n",
    "        level_nodes = []\n",
    "\n",
    "        for _ in range(level_size):\n",
    "            node = queue.popleft()\n",
    "            level_nodes.append(node.val)\n",
    "            if node.left:\n",
    "                queue.append(node.left)\n",
    "            if node.right:\n",
    "                queue.append(node.right)\n",
    "\n",
    "        if not left_to_right:\n",
    "            level_nodes.reverse()\n",
    "        result.append(level_nodes)\n",
    "        left_to_right = not left_to_right\n",
    "    return result\n",
    "```\n",
    "**Explanation**: Standard BFS with a boolean toggle to reverse each alternate level.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [Lowest Common Ancestor of a Binary Tree](https://leetcode.com/problems/lowest-common-ancestor-of-a-binary-tree/)\n",
    "```python\n",
    "def lowestCommonAncestor(root, p, q):\n",
    "    if not root or root == p or root == q:\n",
    "        return root\n",
    "\n",
    "    left = lowestCommonAncestor(root.left, p, q)\n",
    "    right = lowestCommonAncestor(root.right, p, q)\n",
    "\n",
    "    if left and right:\n",
    "        return root\n",
    "    return left if left else right\n",
    "```\n",
    "**Explanation**: Use postorder DFS. If one node found in each subtree, current root is LCA.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Linked Lists\n",
    "\n",
    "### 🔗 [Add Two Numbers](https://leetcode.com/problems/add-two-numbers/)\n",
    "```python\n",
    "class ListNode:\n",
    "    def __init__(self, val=0, next=None):\n",
    "        self.val = val\n",
    "        self.next = next\n",
    "\n",
    "def addTwoNumbers(l1, l2):\n",
    "    dummy = ListNode(0)\n",
    "    current = dummy\n",
    "    carry = 0\n",
    "\n",
    "    while l1 or l2 or carry:\n",
    "        val1 = l1.val if l1 else 0\n",
    "        val2 = l2.val if l2 else 0\n",
    "        total = val1 + val2 + carry\n",
    "        carry = total // 10\n",
    "\n",
    "        current.next = ListNode(total % 10)\n",
    "        current = current.next\n",
    "\n",
    "        l1 = l1.next if l1 else None\n",
    "        l2 = l2.next if l2 else None\n",
    "\n",
    "    return dummy.next\n",
    "```\n",
    "**Explanation**: Simulate digit-by-digit addition using carry and a dummy head.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [Remove Nth Node From End of List](https://leetcode.com/problems/remove-nth-node-from-end-of-list/)\n",
    "```python\n",
    "def removeNthFromEnd(head, n):\n",
    "    dummy = ListNode(0)\n",
    "    dummy.next = head\n",
    "    fast = slow = dummy\n",
    "\n",
    "    for _ in range(n):\n",
    "        fast = fast.next\n",
    "\n",
    "    while fast.next:\n",
    "        fast = fast.next\n",
    "        slow = slow.next\n",
    "\n",
    "    slow.next = slow.next.next\n",
    "    return dummy.next\n",
    "```\n",
    "**Explanation**: Two-pointer technique. Move `fast` ahead by n, then move both until `fast` reaches end.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Dynamic Programming\n",
    "\n",
    "### 🔗 [Coin Change](https://leetcode.com/problems/coin-change/)\n",
    "```python\n",
    "def coinChange(coins, amount):\n",
    "    dp = [float('inf')] * (amount + 1)\n",
    "    dp[0] = 0\n",
    "\n",
    "    for coin in coins:\n",
    "        for x in range(coin, amount + 1):\n",
    "            dp[x] = min(dp[x], dp[x - coin] + 1)\n",
    "\n",
    "    return dp[amount] if dp[amount] != float('inf') else -1\n",
    "```\n",
    "**Explanation**: Classic unbounded knapsack. Build DP table bottom-up.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [Word Break](https://leetcode.com/problems/word-break/)\n",
    "```python\n",
    "def wordBreak(s, wordDict):\n",
    "    word_set = set(wordDict)\n",
    "    n = len(s)\n",
    "    dp = [False] * (n + 1)\n",
    "    dp[0] = True\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(i):\n",
    "            if dp[j] and s[j:i] in word_set:\n",
    "                dp[i] = True\n",
    "                break\n",
    "    return dp[n]\n",
    "```\n",
    "**Explanation**: Use DP to track whether `s[0:i]` can be segmented using valid dictionary words.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Greedy Algorithms\n",
    "\n",
    "### 🔗 [Task Scheduler](https://leetcode.com/problems/task-scheduler/)\n",
    "```python\n",
    "from collections import Counter\n",
    "\n",
    "def leastInterval(tasks, n):\n",
    "    if n == 0:\n",
    "        return len(tasks)\n",
    "    freq = Counter(tasks)\n",
    "    max_freq = max(freq.values())\n",
    "    tasks_with_max_freq = sum(1 for count in freq.values() if count == max_freq)\n",
    "\n",
    "    intervals = (max_freq - 1) * (n + 1) + tasks_with_max_freq\n",
    "    return max(intervals, len(tasks))\n",
    "```\n",
    "**Explanation**: Use greedy formula based on task frequency and idle gaps.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [Jump Game](https://leetcode.com/problems/jump-game/)\n",
    "```python\n",
    "def canJump(nums):\n",
    "    furthest = 0\n",
    "    for i, jump in enumerate(nums):\n",
    "        if i > furthest:\n",
    "            return False\n",
    "        furthest = max(furthest, i + jump)\n",
    "    return True\n",
    "```\n",
    "**Explanation**: Track the furthest index reachable. If at any index it's not reachable, return False.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Graphs and BFS/DFS\n",
    "\n",
    "### 🔗 [Number of Islands](https://leetcode.com/problems/number-of-islands/)\n",
    "```python\n",
    "def numIslands(grid):\n",
    "    if not grid:\n",
    "        return 0\n",
    "    rows, cols = len(grid), len(grid[0])\n",
    "    count = 0\n",
    "\n",
    "    def dfs(r, c):\n",
    "        if r < 0 or c < 0 or r >= rows or c >= cols or grid[r][c] != '1':\n",
    "            return\n",
    "        grid[r][c] = '0'\n",
    "        dfs(r+1, c)\n",
    "        dfs(r-1, c)\n",
    "        dfs(r, c+1)\n",
    "        dfs(r, c-1)\n",
    "\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            if grid[r][c] == '1':\n",
    "                count += 1\n",
    "                dfs(r, c)\n",
    "\n",
    "    return count\n",
    "```\n",
    "**Explanation**: DFS to mark visited land. Count how many times you start a new DFS.\n",
    "\n",
    "---\n",
    "\n",
    "### 🔗 [Course Schedule](https://leetcode.com/problems/course-schedule/)\n",
    "```python\n",
    "from collections import deque\n",
    "\n",
    "def canFinish(numCourses, prerequisites):\n",
    "    graph = {i: [] for i in range(numCourses)}\n",
    "    indegree = [0] * numCourses\n",
    "\n",
    "    for course, prereq in prerequisites:\n",
    "        graph[prereq].append(course)\n",
    "        indegree[course] += 1\n",
    "\n",
    "    queue = deque([i for i in range(numCourses) if indegree[i] == 0])\n",
    "    taken = 0\n",
    "\n",
    "    while queue:\n",
    "        curr = queue.popleft()\n",
    "        taken += 1\n",
    "        for neighbor in graph[curr]:\n",
    "            indegree[neighbor] -= 1\n",
    "            if indegree[neighbor] == 0:\n",
    "                queue.append(neighbor)\n",
    "\n",
    "    return taken == numCourses\n",
    "```\n",
    "**Explanation**: Use topological sorting via BFS (Kahn's algorithm) to detect cycles.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Stacks and Queues\n",
    "\n",
    "### 🔗 [Decode String](https://leetcode.com/problems/decode-string/)\n",
    "```python\n",
    "def decodeString(s):\n",
    "    num_stack = []\n",
    "    str_stack = []\n",
    "    current_str = \"\"\n",
    "    current_num = 0\n",
    "\n",
    "    for ch in s:\n",
    "        if ch.isdigit():\n",
    "            current_num = current_num * 10 + int(ch)\n",
    "        elif ch == '[':\n",
    "            num_stack.append(current_num)\n",
    "            str_stack.append(current_str)\n",
    "            current_num = 0\n",
    "            current_str = \"\"\n",
    "        elif ch == ']':\n",
    "            repeat = num_stack.pop()\n",
    "            prev = str_stack.pop()\n",
    "            current_str = prev + current_str * repeat\n",
    "        else:\n",
    "            current_str += ch\n",
    "\n",
    "    return current_str\n",
    "```\n",
    "**Explanation**: Use stacks to decode nested patterns like `3[a2[b]]` recursively.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Becoming a Top-Tier Data Engineer (PySpark & Azure Databricks)\n",
    "\n",
    "**Summary:** This comprehensive guide will equip you with the technical and soft skills needed to excel as a top-tier Data Engineer, with a focus on **PySpark** and **Azure Databricks**. We’ll cover core data engineering concepts, PySpark optimizations, Databricks-specific features (clusters, workflows, security), CI/CD and DevOps practices, orchestration tools, modern data storage/format technologies (Delta Lake, Parquet, ADLS Gen2), and essential soft skills for collaborating in data teams. **Each section provides structured insight and recommended learning resources (books, courses, docs) to deepen your mastery.**\n",
    "\n",
    "<br>\n",
    "\n",
    "## 1. Core Data Engineering Concepts\n",
    "\n",
    "**Goal:** Understand foundational concepts in data engineering, including **data modeling**, **data warehousing**, and **data pipelines** for both batch and streaming data.\n",
    "\n",
    "- **Data Modeling Paradigms:** Familiarize yourself with different modeling techniques: \n",
    "  - **Third Normal Form (3NF):** Highly normalized relational schema, minimizes data duplication but can be rigid for analytics. Often used in OLTP systems and staging layers of warehouses.\n",
    "  - **Dimensional Modeling (Star Schema):** Kimball-style **fact** and **dimension** tables optimized for query performance and ease of use in analytical systems (OLAP). Facts are numeric measurements, dimensions provide context (e.g., time, product, location).\n",
    "  - **Data Vault:** A hybrid modeling technique using **Hubs (business keys)**, **Links (relationships)**, and **Satellites (attributes)** to achieve **scalability** and **historical tracking**. Data Vault is agile and can adapt to changing business requirements with minimal rework.\n",
    "  - **Lakehouse (Medallion Architecture):** A modern approach that layers data into **Bronze (raw)**, **Silver (cleaned/aggregated)**, **Gold (feature/serving)** layers using data lakes and Delta Lake. This combines the reliability of warehouses and flexibility of lakes.\n",
    "\n",
    "- **Data Warehousing Best Practices:** Build robust **enterprise data warehouses (EDWs)**:\n",
    "  - **Clear Goals & Stakeholder Involvement:** Define business objectives and involve end-users early to ensure the warehouse meets analytical needs.\n",
    "  - **Governance and Data Quality:** Implement data governance processes (quality checks, lineage tracking, master data management) so that “garbage in” doesn’t lead to “garbage out”.\n",
    "  - **Schema Design:** Use the appropriate schema for the use case. **Star schemas** are user-friendly and performant for read-heavy analytics, whereas **3NF** might suit an **ODS (Operational Data Store)**. **Data Vault** can feed a dimensional warehouse as an upstream model.\n",
    "  - **Documentation:** Document the warehouse schema, table purposes, and data refresh schedules. This helps onboard new team members and aligns understanding across business and technical teams.\n",
    "\n",
    "- **ETL vs. ELT Pipelines:** Know the difference:\n",
    "  - **ETL (Extract-Transform-Load):** Traditional approach where data is **transformed on a secondary server** and then loaded into the warehouse. Good for legacy systems or strict transformation needs up front. However, can be slower for very large datasets since transformation isn’t leveraging MPP (Massively Parallel Processing) of modern platforms.\n",
    "  - **ELT (Extract-Load-Transform):** Modern approach (especially with cloud data warehouses) where raw data is quickly **loaded** into a lake or warehouse, and transformations happen **in-place** using the warehouse’s compute power. **ELT is typically faster and more flexible** for large datasets, and leverages pushdown (SQL engines) for transformation. It’s the standard for cloud-based lakehouse architectures.\n",
    "  - *Resource:* *“What’s the Difference Between ETL and ELT?”* on AWS , which explains how ELT leverages modern cloud warehouses for parallel transformation and often results in simpler pipelines.\n",
    "\n",
    "- **Batch vs. Streaming Data Processing:** Understand use cases and tools for each:\n",
    "  - **Batch Processing:** Processes large volumes of data in **discrete batches** (e.g., daily ETL jobs). Suitable when **results aren’t time-sensitive** (end-of-day reports, monthly aggregations). Simpler error handling (you can re-run a batch) but **higher latency**.\n",
    "  - **Stream Processing:** Continuous processing of data as it arrives, enabling **real-time analytics**. Use cases include fraud detection, IoT sensor analytics, real-time user personalization. Requires frameworks like Apache Spark Structured Streaming, Apache Flink, or Kafka Streams. Emphasize handling **event time vs processing time, windowing, and idempotency** in streaming.\n",
    "  - **Key Differences:** Batch offers **throughput** and simpler design at the cost of latency; streaming provides **low latency** and continuous insights at the cost of complexity (ordering, exactly-once processing, etc.).\n",
    "  - *Resource:* *“Batch vs. Stream Processing: Key Differences Explained (2025)”* by Atlan – describes pros/cons and when to use each.\n",
    "\n",
    "- **Orchestrating Data Pipelines:** Architect end-to-end workflows:\n",
    "  - **Pipeline Architecture:** Separate **ingestion**, **processing**, and **serving** layers. For example, ingest raw logs to a **Bronze** Delta table, refine and join into **Silver** tables, then aggregate into **Gold** tables for dashboards.\n",
    "  - **Workflow Orchestration:** Use tools (discussed in Section 5) to manage dependencies – e.g., ensure the Silver job runs after Bronze ingestion. Design pipelines to be **idempotent** (safe to re-run) by partitioning data or using **upserts** (for which Delta Lake’s ACID properties are ideal).\n",
    "  - **Batch vs Streaming Integration:** Many systems use a **lambda architecture** (separate batch and real-time paths converging in output) or **delta architecture** (primarily streaming with occasional batch correction). As a data engineer, you should know how to combine these when needed or move towards a **unified pipeline** with modern tools (e.g., Delta Live Tables).\n",
    "\n",
    "> **Learning Resources – Core Concepts:**  \n",
    "> • *“Fundamentals of Data Engineering” by Joe Reis & Matt Housley* – an excellent book covering modern DE practices end-to-end (planning, pipelines, data governance, etc.) – highly recommended.  \n",
    "> • *“The Data Warehouse Toolkit” by Ralph Kimball* – the classic guide on dimensional modeling. Even though some parts are dated, the core dimensional design principles (star schema, slowly changing dimensions) are timeless.  \n",
    "> • *Coursera:* “Data Warehousing for Business Intelligence” (University of Colorado) – a course focusing on data modeling and warehousing concepts.  \n",
    "> • *Databricks Academy:* “Lakehouse Fundamentals” – covers the medallion architecture and how data lakes and warehouses converge on Databricks.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 2. PySpark Mastery \n",
    "\n",
    "**Goal:** Deep dive into **PySpark** (Spark’s Python API) – advanced transformations, performance tuning, debugging, and ecosystem integration. Apache Spark is central to big data engineering, and PySpark allows you to harness Spark’s power with Pythonic ease.\n",
    "\n",
    "- **Spark Execution Model:** Understand **Spark’s lazy evaluation and DAG (Directed Acyclic Graph) execution**. When you perform transformations (e.g., `filter`, `select`), Spark builds a DAG. An action (like `count()` or writing data) triggers execution. The **job** breaks into **stages** separated by **shuffle boundaries**, and each stage has tasks working on partitions of data.\n",
    "  - *Tip:* Fewer shuffles = faster jobs. Use the DAG visualization in Spark UI (`spark://<driver-node>:4040`) to identify bottlenecks. Look for the **“Stages”** tab to see how many tasks, how much data was shuffled, etc.\n",
    "  - **Common Transformations:** Ensure you’re comfortable with `map`, `flatMap`, `groupByKey` vs `reduceByKey` (prefer the latter for combiners), `join` types (inner, left, broadcasts).\n",
    "  - **Wide vs Narrow Dependencies:** Narrow (e.g., `map`, `filter`) means one output partition depends on one input partition, while wide (e.g., `groupByKey`, `join`) causes shuffles. This concept is crucial for performance tuning.\n",
    "\n",
    "- **Advanced Transformations & Actions:** \n",
    "  - **Window Functions:** Use PySpark SQL’s window functions for tasks like ranking, sessionization, or calculating moving averages. They let you perform calculations across a “window” of input rows, often more efficiently than manual groupings. *(Example:* `from pyspark.sql.window import Window` and using `windowSpec = Window.partitionBy(\"category\").orderBy(\"sales\")` with functions like `rank()`*.)*\n",
    "  - **Aggregations & Joins:** Learn to leverage Spark SQL functions (`spark.sql.functions`) for efficient aggregations. Be mindful of join strategies:\n",
    "    - Use **broadcast joins** when one side of join is small enough to fit in memory (hint Spark with `broadcast(small_df)` or set auto-broadcast threshold).\n",
    "    - If experiencing data skew, consider techniques like **salting** (adding a random number to keys to spread out skewed keys) or using DataFrame API’s `hint(\"skew\")` in Spark 3.  \n",
    "  - **UDFs and Pandas UDFs:** Regular Python UDFs can be **performance killers** due to serialization overhead. Prefer Spark’s built-in functions or use **vectorized UDFs (Pandas UDFs)** which use Apache Arrow for efficient data transfer between JVM and Python.\n",
    "\n",
    "- **Optimizing PySpark Performance:**  \n",
    "  - **Partitioning:** Proper partitioning ensures data is evenly distributed and minimizes shuffles:\n",
    "    - Use **`repartition(n)`** to increase partitions (costly – involves shuffle) when you need more parallelism (e.g., reading a small file that results in a single partition). \n",
    "    - Use **`coalesce(n)`** to reduce partitions without a full shuffle (e.g., coalesce after heavy filtering).\n",
    "    - **Partition Pruning:** When writing data, partition by a logical key (like date or category) if it helps skipping data on reads. For example, writing Parquet with `partitionBy(\"date\")` means queries filtering on date only read relevant partitions.\n",
    "  - **Caching:** If you reuse the same DataFrame multiple times (especially in iterative algorithms or multiple actions in one job), use `.cache()` or `.persist()` to avoid recomputation ([Optimizing PySpark Jobs: Best Practices and Techniques | by Devendra | Medium](https://medium.com/@devendra631995/optimizing-pyspark-jobs-best-practices-and-techniques-7308d2a99071#:~:text=However%2C%20caching%20consumes%20memory%2C%20so,will%20be%20reused%20multiple%20times)). Check Spark UI storage tab to ensure caching happened, and beware of memory limits (persist to disk if needed with `persist(StorageLevel.MEMORY_AND_DISK)`).\n",
    "    - Only cache **what is reused**; caching large data that’s used once can waste memory ([Optimizing PySpark Jobs: Best Practices and Techniques | by Devendra | Medium](https://medium.com/@devendra631995/optimizing-pyspark-jobs-best-practices-and-techniques-7308d2a99071#:~:text=However%2C%20caching%20consumes%20memory%2C%20so,will%20be%20reused%20multiple%20times)).\n",
    "  - **Joins and Shuffles:** Avoid shuffles by:\n",
    "    - Using **map-side combines** (`reduceByKey` vs `groupByKey`) to reduce data before shuffle.\n",
    "    - **Sorting** dataframes on join keys and using `sortMergeJoin` (Spark does this for large joins by default) – ensure the join keys have the same partitioning if possible.\n",
    "    - **Skew mitigation:** Identify skew via Spark UI (one task taking much longer). Use salting for joins on skewed keys or **broadcast the smaller side** if one side is extremely small compared to the other.\n",
    "  - **Memory Management:** Python can be heavy; use Spark configuration to your advantage:\n",
    "    - Enable **Arrow UDFs** for converting Spark DataFrames to Pandas (`spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", True)`).\n",
    "    - Consider the **G1GC** garbage collector for large heap if using Java 11.\n",
    "    - Avoid very large partitions (in general, aim for 100MB – 1GB per file in HDFS/ADLS for efficiency). Too small = too many tasks; too large = out of memory risk.\n",
    "    - *Resource:* Spark’s official *“Performance Tuning”* guide for configuration settings like `spark.sql.shuffle.partitions` (default 200, often too high for small jobs or too low for big jobs) and `spark.default.parallelism`.\n",
    "\n",
    "  - **Adaptive Query Execution (AQE):** In Spark 3+, enabling AQE (`spark.sql.adaptive.enabled=true`) lets Spark **optimize at runtime**, e.g., **dynamically coalescing shuffle partitions** and **switching join strategies** for skewed data. Azure Databricks sets AQE off by default (as of 2025) but enabling it often yields performance gains.\n",
    "\n",
    "- **Debugging & Testing PySpark Jobs:**  \n",
    "  - **Spark UI & Logs:** Use the Spark UI for **performance debugging** (e.g., time taken per stage, skew detection) and **error checking**. The **“Exceptions”** section in Spark UI can show the stacktrace for failed tasks. Also, print schema often (`df.printSchema()`) to ensure the data types are as expected, especially after complex transformations.\n",
    "  - **Logging:** Leverage logging within jobs (`log4j` or Python `logging`) to emit progress. Databricks notebooks allow `print()` to stdout, but in production jobs prefer structured logging (which can be forwarded to Azure Monitor – see Section 3 Monitoring).\n",
    "  - **Unit Testing:** It’s tricky due to distributed nature, but tools like **[Chispa](https://github.com/MrPowers/chispa)** make it easier to test PySpark code. Chispa provides helper methods to assert DataFrame equality and check columns with meaningful error messages. You can write tests that create small DataFrames, apply your transformation function, and compare to expected DataFrames.\n",
    "    - Use **pytest** with a local Spark session (`SparkSession.builder.master(\"local[1]\")`) for unit tests of PySpark. Alternatively, use **Databricks Connect** or the new **Databricks CLI & Bundles** to run tests in a CI pipeline.\n",
    "    - *Resource:* *“Testing PySpark Code”* by Matthew Powers – demonstrates using `chispa` for DataFrame comparisons. Also see Databricks doc: *“Unit testing for notebooks”*.\n",
    "  - **Notebook vs IDE:** While Databricks notebooks are great for interactive development, serious projects should be developed in modular Python scripts or packages and tested with standard tools. Databricks Repos allows syncing notebooks with Git for version control.\n",
    "\n",
    "- **PySpark Integration with Other Ecosystems:**  \n",
    "  - **Delta Lake:** PySpark on Databricks seamlessly integrates with **Delta Lake** (just use `.format(\"delta\")`). Learn Delta’s features: ACID transactions, time travel (`df.history()`), and **OPTIMIZE** command for file compaction to tackle small files ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=Small%20files%20are%20problematic%20because,by%20the%20small%20file%20overhead)) ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=Small%20files%20are%20problematic%20because,by%20the%20small%20file%20overhead)). Delta is now open source; you can also use it off Databricks.\n",
    "  - **Spark & Pandas:** Use **pandas API on Spark** (Spark 3.2+) or **Koalas** (if on older version) for pandas-like syntax on big data. This is useful for data scientists familiar with pandas but needing scale.\n",
    "  - **Connectors:** PySpark can connect to many sources (JDBC for databases, cloud storage, etc.). E.g., use the **Spark JDBC** for reading from an RDBMS (with partitioning options to parallelize reads). Also, learn about **Spark-optimized formats** (Parquet, ORC) which provide predicate pushdown and column pruning for efficiency.\n",
    "\n",
    "> **Learning Resources – PySpark:**  \n",
    "> • *“Spark: The Definitive Guide” by Bill Chambers & Matei Zaharia* – comprehensive coverage of Spark (with Scala/Java focus but concepts apply to PySpark).  \n",
    "> • *“Learning Spark, 2nd Edition” by Jules Damji et al.* – covers Spark 3.x features including Delta Lake and Structured Streaming. Available as a free PDF from Databricks.  \n",
    "> • Databricks Academy course: *“Apache Spark Programming with Databricks”* – focuses on using PySpark in Databricks environment, includes labs on optimization.  \n",
    "> • *YouTube:* “Best Practices for Optimizing Spark Jobs” – many conference talks (Spark Summit) on tuning techniques (cache, partitioning, join optimization). Databricks has a great blog series on [Spark performance tuning](https://databricks.com/blog/2022/05/16/optimize-apache-spark-performance.html).\n",
    "\n",
    "<br>\n",
    "\n",
    "## 3. Azure Databricks Deep Dive\n",
    "\n",
    "**Goal:** Harness the full capabilities of **Azure Databricks** (ADB), a cloud-based Spark platform. Topics include cluster management, Databricks Workflows (jobs), **Unity Catalog & Lakehouse security**, and monitoring.\n",
    "\n",
    "- **Cluster Management & Autoscaling:**  \n",
    "  - **Cluster Modes:** Azure Databricks offers **Standard clusters** and **High Concurrency clusters** (for serving multiple users, configured with SQL endpoints etc.). Also, **Job clusters** (ephemeral clusters tied to jobs) vs **Interactive clusters** (for notebooks). Choose appropriately: use job clusters for production jobs (auto-terminate to save cost), interactive for exploration.\n",
    "  - **Autoscaling:** Databricks can **autoscale** worker nodes between a min and max range. **Optimized Autoscaling** (Azure Databricks’ adaptive algorithm) can scale **up quickly** when backlog spikes and **down gradually** to avoid thrashing. Autoscaling is great for cost-saving on spiky workloads, but for very consistent heavy loads, a fixed-sized cluster may be more stable and faster.\n",
    "    - *Best Practice:* Set a reasonable **min_workers** (so cluster doesn’t scale to zero if you expect frequent work) and a **max_workers** based on workload parallelism needs. Monitor over time and adjust. Autoscaling clusters are not necessarily the fastest for SLA-sensitive jobs, but usually most cost-efficient.\n",
    "    - *Advanced:* If using **Delta Live Tables (DLT)** or structured streaming, Databricks has *Enhanced Autoscaling* tuned for streaming workloads.\n",
    "  - **Cluster Sizing:** Pay attention to **driver vs worker size**. The driver should not be a bottleneck (e.g., if you have a huge cluster but a tiny driver, the driver can run out of memory on job planning). **Photon-enabled clusters** (on runtimes that support it) can greatly speed up SQL and DataFrame operations by using Databricks’ native vectorized engine ([Best practices for performance efficiency - Azure Databricks | Microsoft Learn](https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/performance-efficiency/best-practices#:~:text=Use%20native%20platform%20engines)).\n",
    "  - **Instance Types:** On Azure, leverage **DBU cost vs performance** – sometimes using fewer larger instances is better than many small ones (less overhead in shuffles). However, a larger number of smaller nodes might start up faster and give more parallelism for small tasks. Consider **spot instances** for non-critical jobs to save cost (Databricks will replace lost spot VMs automatically).\n",
    "  - **Serverless Pools:** Azure Databricks has a serverless option for SQL and (in preview) for workloads – this abstracts away the VMs and auto-scales without explicit cluster setup. Use serverless for quick start times and fully managed autoscaling (especially for ad-hoc or BI workloads).\n",
    "\n",
    "- **Job Orchestration with Databricks Workflows:**  \n",
    "  - **Databricks Jobs/Workflows:** A **Databricks Job** can consist of a single task (notebook, JAR, Python script) or a **DAG of tasks** (the new Workflows feature). Use the Jobs UI to create multi-task jobs with dependencies, or use JSON job definitions for Infra-as-Code deployment.\n",
    "  - **Task Orchestration Features:** \n",
    "    - You can specify **task dependencies** (Task B runs after Task A). \n",
    "    - Supports conditional execution, retries, and even branching with DBUtils Exit commands.\n",
    "    - Notifications on failures or SLA breaches: Databricks now supports setting **job failure alerts** and **duration SLA alerts**, emailing or notifying on a Slack/Webhook.\n",
    "    - **Parent-child workflows:** You can have one job call another (jobs can be triggered from another job task). This was a gap historically (led people to use Airflow for cross-job orchestration) but now it’s available.\n",
    "  - **Delta Live Tables (DLT):** If your pipelines are SQL or standard ETL, DLT is a managed framework within Databricks for pipeline orchestration with built-in monitoring, quality enforcement, and automation. It’s an option to simplify complex pipeline orchestration (but is an extra cost).\n",
    "  - **Integration with External Orchestrators:** Databricks Workflows are powerful, but you may still coordinate with tools like **Apache Airflow** or **Azure Data Factory** for enterprise scheduling or cross-system workflows:\n",
    "    - Airflow has a Databricks [operator and hooks] – enabling triggering of Databricks jobs from Airflow DAGs. This provides central orchestration if you use Airflow for other tasks (like data quality checks with Great Expectations, etc.).\n",
    "    - Azure Data Factory can start Databricks notebooks via activity. However, if most of your logic is in Databricks, many engineers find sticking to Databricks Workflows simpler (avoid the “pipeline in a pipeline”).\n",
    "    - **Event-Driven triggers:** Databricks Workflows support file arrival triggers (e.g., S3/ADLS Gen2 new file events), which is very useful for event-based pipelines without needing external tools. You can also trigger jobs via the REST API or a webhook for custom event triggers.\n",
    "  - *Resource:* *“Databricks Workflows: Orchestration Made Easy”* (Medium by Matt Weingarten) – explains how new features like SLA notifications and task chaining enable internal orchestration.\n",
    "\n",
    "- **Unity Catalog & Lakehouse Security:**  \n",
    "  Azure Databricks’ **Unity Catalog** is a game-changer for governance:\n",
    "  - **Centralized Metadata & Governance:** Unity Catalog provides a **metastore** that unifies data access across Databricks workspaces. You define **Catalogs** (top-level, often per domain or team) containing **Schemas** (databases) of **Tables/Views**. It allows fine-grained permissions at table, row (via dynamic views), and column (via masking) level.\n",
    "  - **Managed vs External Tables:** With Unity, you can have **managed tables** (Databricks manages storage) or **external tables** (data in your own data lake). Unity Catalog will handle authentication to the underlying storage via configured credentials.\n",
    "  - **Security Best Practices:** \n",
    "    - **Least Privilege:** Configure ACLs such that users have only the access they need (e.g., developers can read data, but only data stewards can write to certain sensitive tables).\n",
    "    - **Service Principals:** Use service principals (app identities) for production jobs and assign them rights on Unity Catalog objects (as opposed to personal user tokens).\n",
    "    - **Data Lineage:** Unity Catalog can capture lineage of queries – which table was produced from which source tables – enabling traceability for compliance and debugging.\n",
    "    - **Column-Level Security:** Use Unity Catalog’s column masking or row filtering for PII. E.g., mask emails except for certain roles. Unity Catalog integrates with **Azure AD** for identity federation and supports SCIM for provisioning groups.\n",
    "    - Unity Catalog also is the foundation for **Delta Sharing**, an open protocol to share data outside your org securely (if needed).\n",
    "  - **Network Security:** Lock down the Databricks control plane and workspace:\n",
    "    - Use **VNet injection** (deploy workspace in your VNet) and enable **Private Link** so that cluster nodes don’t have public IPs.\n",
    "    - **IP Access Lists:** Control which IPs can access the workspace endpoints.\n",
    "    - **Customer-Managed Keys:** For encryption at rest beyond the default (customer-managed keys in Azure Key Vault can be used for E2 workspaces).\n",
    "    - *Resource:* *“Unity Catalog Best Practices”* (Databricks docs) – details catalog/schemas design, recommends a single Unity Catalog for an org and segregating data by catalogs (e.g., FINANCE catalog, MARKETING catalog, etc.).\n",
    "\n",
    "- **Monitoring and Logging:** A production-grade data platform must be observable:\n",
    "  - **Azure Databricks native monitoring:** \n",
    "    - **Cluster Metrics:** The “Metrics” tab on clusters shows CPU, memory, and storage utilization for driver and workers (for the last 60 seconds in UI).\n",
    "    - **Ganglia** metrics (built-in on clusters) for low-level monitoring can be accessed if needed (Databricks provides some endpoints, but in Azure it's limited). Instead, prefer integration with Azure Monitor.\n",
    "  - **Azure Monitor integration:** Use the **Azure Databricks Monitoring Library** to send logs and metrics to Azure Monitor (Log Analytics workspace). This involves attaching an init script and configuring spark metrics. Then:\n",
    "    - **Spark Metrics:** You can capture Spark-level metrics (e.g., number of jobs, shuffle bytes) and **streaming query metrics** to Azure Monitor.\n",
    "    - **Driver & Executor Logs:** Configure cluster to send STDOUT, STDERR, and log4j logs to a storage or directly to Log Analytics. Azure Databricks has a diagnostic logging feature (via workspace settings) to forward logs.\n",
    "    - Set up **Log Analytics queries** or use pre-built dashboards to monitor job failures, error keywords, etc., in logs.\n",
    "  - **Databricks CLI & REST API for monitoring:** Schedule the execution of `databricks jobs list` or use the Jobs API to programmatically check for failed runs, etc., if not using Azure Monitor.\n",
    "  - **Alerting:** On Azure, you can set alerts in Log Analytics (e.g., trigger an alert if any job fails with error X, or if cluster utilization is above 90% for 10 minutes). Additionally, within Databricks, configure **Job notifications** for email on failure (in the Jobs UI).\n",
    "  - *Resource:* *“Monitoring Azure Databricks”* (Azure Architecture Center) – outlines how to send application logs and metrics to Azure Monitor, complete with a reference implementation.\n",
    "\n",
    "> **Learning Resources – Azure Databricks:**  \n",
    "> • *Microsoft Learn:* **“Azure Databricks Documentation”** – especially sections on [Best Practices](https://learn.microsoft.com/en-us/azure/databricks/best-practices), [Security](https://learn.microsoft.com/en-us/azure/databricks/security/), and [Data Governance](https://learn.microsoft.com/en-us/azure/databricks/lakehouse-architecture/data-governance/best-practices).  \n",
    "> • *Databricks Academy:* Courses like **“Administration Essentials”** (for Unity Catalog, workspace setup) and **“Scaling Data Pipelines”**.  \n",
    "> • *Blogs:* *“13 Ways to Optimize Databricks Autoscaling”* (overcast.blog) – advanced autoscaling tips, and *Databricks Engineering Blog* for how they built Photon, etc.  \n",
    "> • *YouTube:* “Azure Databricks Best Practices” (Microsoft Mechanics video) – a quick overview of cluster and workspace optimizations.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 4. CI/CD and DevOps for Data Engineering\n",
    "\n",
    "**Goal:** Apply **DevOps practices** to data engineering projects, ensuring code, configurations, and infrastructure are version-controlled, tested, and deployed reliably. This covers **Git**, automated CI/CD pipelines (Azure DevOps, GitHub Actions), Infra as Code, and testing strategies.\n",
    "\n",
    "- **Version Control with Git:**  \n",
    "  - Treat your data pipelines (notebooks, PySpark jobs, SQL scripts) like software code. Use Git for version control:\n",
    "    - **Repo Structure:** For Databricks, you can use **Databricks Repos** which allow a Git repo to sync with the workspace. Organize code into directories (e.g., `pipelines/`, `notebooks/`, `tests/`, `infra/`).\n",
    "    - **Branching Strategy:** Have feature branches and a main branch (or dev/staging/prod branches) for promotion. Enforce pull requests and code reviews for any production pipeline changes.\n",
    "    - **Notebook Versioning:** If using notebooks, consider exporting them as `.py` files (DBC or source format) for Git storage to avoid JSON notebook conflicts. Or leverage the **%pip** magic to include code from a repository in notebooks.\n",
    "\n",
    "- **CI Pipelines (Build & Test):**  \n",
    "  - Automate testing of your data pipeline code on each commit:\n",
    "    - **GitHub Actions** or **Azure Pipelines** can run **pytest** to execute unit tests on PySpark code. Use a **local Spark** (`local[2]`) for quick tests, or spin up an ephemeral Databricks cluster via API for integration tests (Databricks Connect or Jobs API).\n",
    "    - **Linting & Formatting:** Use pylint/flake8 for code quality, and black for consistent formatting.\n",
    "    - **Churn Tests with Data:** For integration tests, you might use a small **sample dataset** (perhaps stored in a test folder or generated on the fly) to run through the pipeline end-to-end. For example, as part of CI, spin up a job on Databricks that runs your notebooks against a test data set and assert the outputs (using **Great Expectations** for data validation or custom checks).\n",
    "    - **Chispa for Testing:** As mentioned earlier, incorporate chispa for DataFrame checks in unit tests. CI can output any assertion failures with clear diff of DataFrames.\n",
    "\n",
    "  - **Databricks-CI Integration:**  \n",
    "    - Databricks provides a **CLI** and a **REST API**. Use a service principal’s token to authenticate CI to Databricks.  \n",
    "    - **GitHub Actions Example:** Install Databricks CLI in a workflow step, configure with a PAT (Personal Access Token) securely, then use CLI commands (`databricks workspace import_dir`, `databricks jobs deploy`) or the new **Asset Bundles** CLI to deploy code.\n",
    "    - **Azure DevOps:** Use the [Databricks CLI extension or API calls] in a release pipeline. For example, tasks that upload notebooks or wheel files to DBFS, then configure jobs.\n",
    "\n",
    "- **CD Pipelines (Deploy):**  \n",
    "  - **Infrastructure as Code (IaC):** Use Terraform or Azure Resource Manager (ARM/Bicep) to manage cloud resources:\n",
    "    - **Databricks Workspace** itself can be deployed via ARM or Terraform (Databricks provider). Unity Catalog requires setting up via Terraform (metastore, catalogs, etc.).\n",
    "    - **Clusters and Jobs as Code:** Rather than manually clicking UI, define clusters in JSON or via Terraform. The Databricks Terraform provider can manage clusters, jobs, and even permissions.\n",
    "    - *Example:* Terraform script defines a job with a cluster that has autoscaling 2–10 nodes and runs a notebook from a workspace path. Running `terraform apply` in your CD pipeline ensures the job is created/updated in Databricks.\n",
    "  - **Databricks Asset Bundles:** As per Databricks recommendation, Asset Bundles (now GA as of 2025) let you define a YAML with all jobs, notebooks, libraries, etc., and deploy it in one command. This is worth exploring to simplify CI/CD.\n",
    "  - **Release Promotion:** If you have dev/staging/prod workspaces, automate promotion: e.g., after tests pass in dev, your CD pipeline deploys the code to staging workspace, runs a smoke test job, then to prod workspace.\n",
    "    - Alternatively, a single workspace can have jobs parameterized for env (not often, better to separate workspaces for isolation).\n",
    "  - **Library Dependencies:** Package shared code as wheel/jar and attach to clusters via CI. Manage these in a artifacts feed (Azure Artifacts or an internal PyPI).\n",
    "  - **Unit & Integration Testing in CI/CD:** Ensure the pipeline runs **unit tests** (no Spark or local Spark) for quick feedback, and **integration tests** (maybe nightly) on a real cluster with larger data. This two-level testing catches issues early and ensures end-to-end reliability.\n",
    "\n",
    "- **Continuous Deployment with Quality Gates:**  \n",
    "  - Use pull request checks (branch policies in Azure Repos or required checks in GitHub) to block merging code that fails tests or lint.\n",
    "  - Code coverage tools (like Coverage.py) can show how much of your pipeline code is tested.\n",
    "  - If using Azure DevOps, consider **Manual Intervention gates** for prod deploy (so someone approves a production job update after seeing it run in staging).\n",
    "\n",
    "- **DevOps for DataOps:**  \n",
    "  - Recognize challenges: Data pipelines output data, not just code artifacts. That means part of “deployment” is ensuring the **data is compatible**. For instance, if you change a schema, your pipeline might deploy fine but break existing reports. Mitigate this with:\n",
    "    - Data Contracts and backward compatibility checks.\n",
    "    - Feature flags or conditional logic if new vs old data.\n",
    "    - Blue/Green deployment strategies for pipelines (less common, but e.g., write new results to a new table while keeping old table until validation is done).\n",
    "  - **MLOps Integration:** If models are part of pipeline, integrate with MLflow Model Registry and treat model promotions similar to code (review metrics before promoting a model to “Production” stage in MLflow).\n",
    "\n",
    "> **Learning Resources – CI/CD & DevOps:**  \n",
    "> • *Azure Databricks CI/CD Guide (Microsoft Docs)* – covers using Azure DevOps and GitHub Actions for Databricks, including a walkthrough with Databricks Repos and CLI.  \n",
    "> • *“The Ultimate Guide to CI/CD in Databricks” by Eduard Popa* (Medium) – very insightful on challenges and best practices (e.g., environment management, testing data pipelines).  \n",
    "> • *YouTube:* “CI/CD for Data Engineering on Databricks” – look for talks by Databricks or community (e.g., Data + AI Summit sessions on CI/CD).  \n",
    "> • *Book:* \"Data Engineering Teams\" by Jesse Anderson – covers team processes and DevOps culture in data engineering.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 5. Orchestration Tools (Airflow, etc.)\n",
    "\n",
    "**Goal:** Know the landscape of orchestration tools beyond Databricks Workflows, such as **Apache Airflow**, and event-driven designs, to schedule and manage complex pipelines with dependencies and SLAs.\n",
    "\n",
    "- **Apache Airflow:** The most popular open-source workflow orchestrator:\n",
    "  - **Directed Acyclic Graphs (DAGs):** Define tasks in Python and their dependencies. Airflow’s rich library of operators allows calling Databricks jobs, running Spark on EMR, executing Python, bash, etc.\n",
    "  - **Use Cases:** \n",
    "    - **Cross-system workflows:** e.g., pull data via API, load to DB, run a Databricks job, then notify via email – all in one Airflow DAG.\n",
    "    - **SLA Monitoring:** Airflow can alert if a DAG run exceeds a certain time.\n",
    "    - **Databricks Integration:** Airflow’s DatabricksSubmitRunOperator can trigger a job on Databricks and monitor it. Using Airflow’s **retry** mechanisms you can handle transient issues.\n",
    "  - **Airflow Best Practices:** Modularize DAG code, use **Variables/Connections** for credentials, and handle secrets via backends like Azure Key Vault. For Data Engineering, ensure tasks output data to durable storage, since Airflow tasks should be idempotent and stateless.\n",
    "  - *Resource:* Astronomer’s guide *“Orchestrate Databricks jobs with Airflow”* explains why teams use Airflow alongside Databricks (e.g., orchestrating across data stack components).\n",
    "\n",
    "- **Databricks Workflows vs. Airflow:**  \n",
    "  - Today, Databricks Workflows has matured (with task dependencies, branching via if/else notebooks, etc.). If your pipelines are mostly within Databricks and you don’t need to orchestrate other systems, using **Databricks Workflows can reduce complexity** (no separate Airflow infra). This also simplifies job scheduling and reduces latency (no handoff).\n",
    "  - However, **Airflow** or **Azure Data Factory** might still be needed if:\n",
    "    - You need a **central orchestrator** for many systems (not just Spark).\n",
    "    - Company standards require it (e.g., a central data platform team mandates all pipelines be registered in Airflow for visibility).\n",
    "  - Many companies use a mix: Databricks Workflows for pure ETL, and Airflow to call those workflows or integrate with other tasks.\n",
    "\n",
    "- **Event-Driven Pipelines:**  \n",
    "  - Modern data architectures often react to events (like files arriving or messages on a queue):\n",
    "    - **Auto Loader (Databricks):** Continuously checks cloud storage for new files and processes them (very useful for incremental ingestion).\n",
    "    - **Azure Event Grid + Logic Apps:** You can trigger a Databricks job via an Event Grid event (file arrival in ADLS) connected to an Azure Function that calls Databricks REST API.\n",
    "    - **Kafka + Spark Streaming:** Use structured streaming to ingest events and process in near real-time instead of batch scheduling.\n",
    "    - Databricks Workflows now support **file arrival triggers natively**, which means you can set up an event-driven job without external glue.\n",
    "\n",
    "- **Dependency and SLA Management:**  \n",
    "  - No matter the tool, have a clear view of pipeline dependencies:\n",
    "    - Use Gantt charts or dependency graphs (Airflow UI provides this) to monitor.\n",
    "    - Document data dependencies (if dataset A is late, which downstream datasets are affected?).\n",
    "  - **SLAs:** If a pipeline must finish by 6am daily, set up monitoring. Databricks now can notify on SLA misses; Airflow has SLA miss callbacks.\n",
    "  - **Restartability:** Orchestrators should be able to retry tasks, but sometimes manual intervention is needed. Have runbooks for how to handle partial failures (e.g., if pipeline fails midway, do you rerun from scratch or from point of failure? This ties back to idempotence.)\n",
    "\n",
    "> **Learning Resources – Orchestration:**  \n",
    "> • Official **Apache Airflow** docs – focus on tutorial and concepts like XComs (passing data between tasks), pools (throttling), and Airflow’s limitations (e.g., not great for high-throughput streaming).  \n",
    "> • *“Data Pipelines with Apache Airflow”* by Bas P. Harenslak & Julian Rutger de Ruiter – a book covering Airflow in data engineering scenarios.  \n",
    "> • *Airflow on Azure:* Astronomer and Microsoft have guides to deploy Airflow on Azure (AKS or as a managed service with MWAA on AWS or Cloud Composer on GCP if needed).  \n",
    "> • Many blog posts compare Airflow, Luigi, Azkaban, etc., but Airflow is the de facto standard now. Also look into **Dagster** and **Prefect** as newer orchestrators with data-aware scheduling.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 6. Additional Tools & Technologies\n",
    "\n",
    "**Goal:** Expand into other technologies that complement PySpark/Databricks in a data platform: file formats, data lakes, warehouses, and ML/Ops frameworks.\n",
    "\n",
    "- **Delta Lake & Parquet:**  \n",
    "  - **Parquet**: A columnar file format highly optimized for Spark. Columnar storage + predicate pushdown = only read needed columns & rows. **Always prefer Parquet or ORC for big data storage** over CSV/JSON (except at ingestion). Parquet also supports compression (snappy by default) and works well with partitioning.\n",
    "  - **Delta Lake**: Built on Parquet, adds:\n",
    "    - **ACID Transactions:** Safe concurrent reads/writes and no partial results – crucial for reliable pipelines.\n",
    "    - **Time Travel:** Query older snapshot by version or timestamp (`df.history()` or `VERSION AS OF` SQL).\n",
    "    - **Schema Evolution/Enforcement:** Can allow or disallow schema changes, and merge schemas as needed.\n",
    "    - **Delta Operations:** `MERGE` (upserts), `UPDATE`, `DELETE` operations on lake data (these generate new files under the hood).\n",
    "    - **OPTIMIZE:** Coalesce small files into larger ones for efficiency ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=Small%20files%20are%20problematic%20because,by%20the%20small%20file%20overhead)) ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=create%20large%20metadata%20transaction%20logs,by%20the%20small%20file%20overhead)) (especially after many small streaming writes). Use with `ZORDER` to cluster data for common queries (e.g., `OPTIMIZE table ZORDER BY (country)` to sort data files by country for quicker filtering).\n",
    "    - *When to use:* Almost always use Delta on Databricks unless you have a requirement to use raw Parquet. Delta’s overhead is low and the benefits (especially for multi-step pipelines and streaming) are huge.\n",
    "    - *Resource:* Databricks blog post *“Delta Lake Small File Compaction”* demonstrates how `OPTIMIZE` works ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=Small%20files%20are%20problematic%20because,by%20the%20small%20file%20overhead)) ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=create%20large%20metadata%20transaction%20logs,by%20the%20small%20file%20overhead)) and why ACID helps in this context.\n",
    "\n",
    "- **Azure Data Lake Storage Gen2 (ADLS2):**  \n",
    "  - ADLS Gen2 is the storage backbone for Azure big data:\n",
    "    - Combines **Blob Storage** scalability with a **Hierarchical Namespace** (HNS) for directory/file structure and Hadoop-compatible APIs.\n",
    "    - **Hadoop Compatible:** You can use ABFS (Azure Blob File System driver) in Spark URI to read data (`spark.read.csv(\"abfss://<container>@<account>.dfs.core.windows.net/…\")`).\n",
    "    - **Security:** Integrates with Azure AD for **OAuth authentication** and supports **POSIX-like ACLs** on files/folders for fine-grained security.\n",
    "    - For Data Engineers, knowing how to manage ADLS Gen2 is key: e.g., using **Azure Storage Explorer** or CLI for data, setting correct folder permissions so that Databricks (via service principal) can read/write.\n",
    "    - **Mounts vs Direct Access:** In Databricks, you can mount ADLS containers to DBFS for simpler paths or use direct `abfss://` paths. Mounts are convenient but require managing secrets. Direct access with service principal passthrough is often easier with Unity Catalog’s managed identities.\n",
    "\n",
    "- **Azure Synapse Analytics (former SQL Data Warehouse):**  \n",
    "  - Sometimes you’ll use Databricks in tandem with Synapse (for serving data to BI tools or doing large relational queries):\n",
    "    - **PolyBase**: a method to load data from ADLS into Synapse (use Databricks to prep data, then Synapse for final schema or complex SQL).\n",
    "    - **Spark vs SQL Pools:** Synapse has its own Spark and an MPP SQL engine. Databricks Spark is typically more advanced (and often faster for Spark workloads). But the Synapse SQL engine is handy for serving data via T-SQL. Often a pattern is: Databricks refines data into Delta, then that data is exposed via Synapse serverless SQL or a Synapse dedicated SQL pool for reporting.\n",
    "    - *Be aware:* There’s overlap between Synapse and Databricks; many organizations choose one or the other for ETL. But as a top engineer, you should know basics of both. E.g., Synapse has **Materialized Views, Result-set caching** – features that pure Spark doesn’t have.\n",
    "\n",
    "- **MLflow & ML Integration:**  \n",
    "  - **MLflow** is integrated in Databricks for **experiment tracking and model registry**. Key points:\n",
    "    - **Tracking:** `mlflow.start_run()` and log metrics/params from training jobs. In Databricks, it auto-logs many things (especially for sklearn, Keras, etc.). This helps keep a record of model training runs.\n",
    "    - **Model Registry:** Register your best models in the registry, which Unity Catalog now integrates with to manage model permissions. You can transition models through stages: Staging → Production.\n",
    "    - As a data engineer, your role might be providing the platform for Data Scientists:\n",
    "      - Setting up feature ETL pipelines (possibly using **Feature Store**).\n",
    "      - Scheduling training jobs in Databricks (perhaps orchestrated with Workflows).\n",
    "      - Using MLflow to version models and even deploying via MLflow’s model serving capabilities.\n",
    "    - **Model Serving on Databricks:** You can serve models as REST endpoints directly. Under the hood, this uses cluster serving endpoints. Ensure scaling and SLA for these if used in production.\n",
    "    - *Resource:* Databricks Learn *“MLflow Model Registry on Databricks”* for how Unity Catalog enhances model governance.\n",
    "\n",
    "- **Azure Ecosystem Tools:**  \n",
    "  - **Azure Data Factory (ADF):** A cloud ETL orchestrator (mostly UI-driven). Less flexible than Airflow or Databricks Workflows, but useful if your organization leans towards a low-code tool for moving data between services. You might trigger Databricks from ADF and use its monitoring.\n",
    "  - **Azure Event Hubs/Kafka:** For streaming data ingestion at scale into Databricks (structured streaming supports Event Hubs via Kafka API).\n",
    "  - **Azure DevOps (Repos & Pipelines):** Many enterprise Azure shops use this for CI/CD. Learn YAML pipelines syntax for multi-stage pipelines if so.\n",
    "  - **Terraform/Bicep:** As mentioned, for IaC – ensure you can read and write basic Terraform for Azure resources (storage, databricks workspace, etc.). HashiCorp’s Databricks provider documentation would be useful.\n",
    "\n",
    "> **Learning Resources – Additional Tech:**  \n",
    "> • *“Delta Lake: The Definitive Guide”* (coming from O’Reilly in the future) – until then, the Delta Lake online documentation and blog posts by Delta Lake contributors (e.g., Matthew Powers’ blog ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=Small%20files%20are%20problematic%20because,by%20the%20small%20file%20overhead)) ([Delta Lake Small File Compaction with OPTIMIZE | Delta Lake](https://delta.io/blog/2023-01-25-delta-lake-small-file-compaction-optimize/#:~:text=create%20large%20metadata%20transaction%20logs,by%20the%20small%20file%20overhead))) are great.  \n",
    "> • *Coursera:* “Scalable Machine Learning on Big Data using Apache Spark” (Databricks course) – ties together Spark + ML.  \n",
    "> • *Microsoft Learn:* Modules on ADLS Gen2 and Synapse (particularly *“Integrate Azure Databricks with Azure Synapse”*).  \n",
    "> • *Videos:* Databricks’ YouTube channel has many short videos on features like Auto Loader, Delta Lake, MLflow, etc., to see these in action.\n",
    "\n",
    "<br>\n",
    "\n",
    "## 7. Soft Skills and Best Practices\n",
    "\n",
    "**Goal:** Develop the soft skills that distinguish top-tier engineers: communication, documentation, teamwork, and solution design thinking.\n",
    "\n",
    "- **Documentation Standards:** \n",
    "  - **Project Documentation:** Maintain a central README or Confluence page for each project that describes the pipeline architecture, data sources, data sinks, schedule, and whom to contact for issues. Include diagrams (e.g., using Mermaid or draw.io: a flow of data from source to lake to warehouse).\n",
    "  - **Code Documentation:** Write clear docstrings for complex functions. In notebooks, use Markdown cells to explain logic in each step. Well-documented code is **“living documentation”** for your team.\n",
    "  - **Data Dictionary:** Document the schema and meaning of important tables (especially gold tables) – what does each column mean, what are the expected values? This helps downstream consumers.\n",
    "  - **Workflow Documentation:** If using Airflow/ADF, ensure pipeline definitions have descriptions. If SLAs exist, note them clearly.\n",
    "\n",
    "- **Communication & Team Collaboration:**  \n",
    "  - **Cross-Functional Work:** Data engineers work with data scientists, analysts, and sometimes DevOps or platform engineers. **Establish regular syncs** to understand their needs and pain points. For example, meet with Data Science to plan how data engineering can optimize a feature pipeline for a new model.\n",
    "  - **Translate Requirements:** Be able to talk to business/data analysts and convert their needs into technical requirements. E.g., a product analyst needs user retention metrics – you might need to build a pipeline that aggregates logins and signups.\n",
    "  - **Share Knowledge:** As you gain expertise, help others via lunch-and-learns, internal wikis. Maybe you discovered a neat trick to speed up Spark – share it!\n",
    "  - **Manage Expectations:** If a request comes that’s very complex or risky, communicate the trade-offs (perhaps suggest simpler alternative solutions first).\n",
    "\n",
    "- **Designing Scalable Solutions:**  \n",
    "  - **Big Picture Thinking:** Don’t just hack a pipeline that works for 100GB if you know next year it’ll be 10TB. Think **scalability** and design with growth in mind. For example, if using RDBMS ingestion, consider a strategy that will work when tables grow (maybe switch to incremental change data capture rather than full extracts).\n",
    "  - **Cost Awareness:** In cloud, be mindful of cost. Use cluster hours wisely (auto-terminate clusters, use spot instances when sensible). A top-tier engineer delivers value *cost-effectively*. Monitor cost metrics (Databricks has a cost breakdown by cluster/job).\n",
    "  - **Reliability:** Design pipelines that are fault-tolerant (e.g., use checkpointing in Spark streaming, idempotent writes). Have alerting on failures so you can quickly address issues. This is part of DataOps best practices.\n",
    "  - **Data Quality and Testing:** Integrate data quality checks (e.g., using libraries like Great Expectations or custom assertions) to ensure your pipeline output is trustworthy. If data quality issues occur upstream, be proactive in flagging them.\n",
    "\n",
    "- **Working in Teams:**  \n",
    "  - **Code Reviews:** Embrace code reviews for all pipeline code changes. A fresh pair of eyes can spot inefficiencies or errors. Be open to feedback and also learn to review others’ code constructively.\n",
    "  - **Agile Methodologies:** Many data teams use Kanban or Scrum. Keep Jira or Azure Boards updated, break tasks into small deliverables, and demo your work to stakeholders regularly to gather feedback.\n",
    "  - **Mentoring and Leadership:** As you grow, mentor junior engineers on the team. This could involve pair programming sessions or guiding them through their first project. Teaching is one of the best ways to solidify your own mastery.\n",
    "\n",
    "- **Estimating & Planning:**  \n",
    "  - **Effort Estimation:** Data projects often involve unknowns (data issues, performance challenges). Gain experience in breaking down tasks and adding contingency. When estimating, consider: data exploration time, dev, testing, backfill of historical data, etc.\n",
    "  - **Capacity Planning:** If responsible for infrastructure, forecast usage. For example, if new IoT devices are coming online, project how ingestion rates will grow and whether you need a bigger cluster or new optimization.\n",
    "  - **SLA Commitments:** If you promise data by 8am daily, ensure you have a plan B if pipeline fails (like automated rerun or manual fix procedure by 7:30am). Communicate early if an SLA might be missed.\n",
    "\n",
    "- **Continual Learning:**  \n",
    "  - The data field evolves quickly (new Spark versions, new tools like Delta Sharing, lakehouse concept updates). Dedicate time to **continuous learning** – read blogs (e.g., AWS Big Data Blog, Databricks Blog, Microsoft Tech Community for Azure), attend meetups or online webinars. Top engineers stay current and experiment with new features (e.g., trying out Databricks MLflow model serving even if your current project doesn’t demand it, just to understand potential).\n",
    "  - Possibly pursue relevant certifications (e.g., **Databricks Certified Data Engineer**, **Azure DP-203 Data Engineering on Azure**). The study process for these certs will reinforce your knowledge in areas you might not touch daily (like Azure Cosmos DB or advanced Spark optimization).\n",
    "\n",
    "> **Learning Resources – Soft Skills:**  \n",
    "> • *“Effective Data Storytelling”* by Brent Dykes – while targeted at analysts, it helps engineers understand how stakeholders consume data.  \n",
    "> • *“Team Topologies”* by Matthew Skelton & Manuel Pais – not data-specific, but great for understanding team interactions (e.g., how a platform team supports a feature team – analogous to how a data platform team might support data science team).  \n",
    "> • *Blogs:* The Seattle Data Guy and Barr Moses (Monte Carlo) often write about data team processes and best practices (e.g., documentation, DataOps culture).  \n",
    "> • *Communication:* Even reading general software engineering books like “Clean Code” (Robert Martin) and “The Pragmatic Programmer” can improve how you structure and present your work.\n",
    "\n",
    "<br>\n",
    "\n",
    "## **Conclusion & Next Steps**\n",
    "\n",
    "Becoming a **top-tier Data Engineer** in the PySpark/Azure Databricks ecosystem involves mastering a breadth of technologies and honing best practices. **Start with the fundamentals**: ensure you solidify your understanding of data modeling and Spark internals. **Gradually incorporate advanced topics** like Delta Lake optimizations, CI/CD automation, and security design. \n",
    "\n",
    "Remember that **soft skills magnify your technical skills** – a well-optimized pipeline still needs clear documentation and communication for others to appreciate and maintain it. Engage with the data community (forums like [r/dataengineering](https://reddit.com/r/dataengineering), Databricks community, local meetups) to learn real-world tips and keep your knowledge up-to-date (the field is now evolving towards lakehouse pattern, data mesh architectures, etc.).\n",
    "\n",
    "By following this guide and leveraging the recommended resources, you’ll build a strong, comprehensive skill set. **Set learning goals for each section** (e.g., implement a mini-project to practice streaming, or write a Terraform script to deploy a Databricks job) and track your progress. In time, you’ll be confidently designing robust data pipelines, optimizing Spark jobs that handle terabytes smoothly, and leading your team by example in data engineering excellence.\n",
    "\n",
    "Good luck on your journey to becoming a top-tier data engineer – **build reliably, optimize relentlessly, and always stay curious**!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
